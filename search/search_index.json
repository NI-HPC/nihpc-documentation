{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The NI-HPC Centre is a UK Tier-2 National High Performance Computing (HPC) facility funded by the Engineering and Physical Sciences Research Council (EPSRC) and jointly managed by Queen's University Belfast (QUB) and  Ulster University. The \u00a32M investment from EPSRC will have significant impact for science, by expanding the use of HPC to new pools of talent and new areas of investigation for Northern Ireland. This will be done by building on the strong collaboration between Ulster University and QUB, already established through two major city deal initiatives.</p> <p>To learn more please visit our website.</p>"},{"location":"#acceptable-use-policy","title":"Acceptable Use Policy","text":"<p>Northern Ireland High Performance Computing (NI-HPC) Centre Acceptable Use Policy</p>"},{"location":"#1-introduction","title":"1. Introduction","text":"<p>This Acceptable Use Policy (AUP) sets forth the terms and conditions under which users are granted access to the Kelvin-2 system based at the NI-HPC Centre at Queen\u2019s University Belfast. The NI-HPC service is provided to support academic research and teaching activities. All users must adhere to this policy to ensure the service remains secure, efficient, and equitable for all. </p>"},{"location":"#2-scope","title":"2. Scope","text":"<p>This policy applies to all users of the NI-HPC service, including but not limited to students, academic staff, researchers, and external collaborators. It covers the use of all NI-HPC resources, including computing nodes, storage systems, networking, and software applications. </p>"},{"location":"#3-user-responsibilities","title":"3. User Responsibilities","text":"<p>3.1 Compliance with Laws and University Policies</p> <p>Users must comply with all relevant UK laws, University regulations, and policies when using the HPC service. This includes, but is not limited to, compliance with the General Data Protection Regulation (GDPR), intellectual property laws, and the University\u2019s IT policies.</p> <p>3.2 Account Security</p> <p>Users are responsible for the security of their NI-HPC accounts. This includes safeguarding passwords, not sharing account credentials, and immediately reporting any suspected security breaches to the NI-HPC support team. </p> <p>3.3 Use of Resources</p> <p>NI-HPC resources must be used efficiently and responsibly. Users should:</p> <ul> <li>Prioritize jobs appropriately and avoid monopolizing resources.</li> <li>Use storage space judiciously and delete data no longer required.</li> <li>Not run unauthorized software or engage in activities that could harm the system's integrity or performance.</li> <li>Adhere to any specific guidelines or restrictions communicated by the NI-HPC team regarding resource usage.</li> </ul> <p>3.4 Data Management and Confidentiality</p> <p>Users are responsible for managing their data, including ensuring regular backups and adhering to data retention policies. Sensitive or confidential data must be handled in accordance with the University's data protection policies. </p>"},{"location":"#4-prohibited-activities","title":"4. Prohibited Activities","text":"<p>The following activities are strictly prohibited: </p> <p>4.1 Unauthorized Access </p> <p>Attempting to gain unauthorized access to any part of the NI-HPC service or other systems connected to the University network. </p> <p>4.2 Malicious Activity </p> <p>Engaging in any activity that could harm the NI-HPC system or other users, including the introduction of malware, denial-of-service attacks, or any form of hacking. </p> <p>4.3 Commercial Use </p> <p>Using the NI-HPC service for commercial purposes without explicit authorization from the University. The HPC service is intended for academic and research purposes only. </p> <p>4.4 Infringement of Intellectual Property </p> <p>Using the NI-HPC service to infringe upon the intellectual property rights of others, including unauthorized use of copyrighted materials, software, or data. </p> <p>4.5 Unlawful or Unethical Use </p> <p>Any use of the NI-HPC service for activities that are unlawful, unethical, or violate University policies, including but not limited to, storing or distributing offensive, discriminatory, or otherwise inappropriate content. </p>"},{"location":"#5-monitoring-and-enforcement","title":"5. Monitoring and Enforcement","text":"<p>5.1 Monitoring </p> <p>The University reserves the right to monitor the use of the NI-HPC service to ensure compliance with this AUP. This includes the monitoring of job submissions, resource usage, and data transfers. Such monitoring will be conducted in compliance with relevant privacy laws and University policies. </p> <p>5.2 Violation and Consequences </p> <p>Violations of this policy may result in the suspension or termination of access to the NI-HPC service, disciplinary action, and potential legal consequences. The University may also recover any costs incurred due to unauthorized use or damage caused by policy violations. </p>"},{"location":"#6-support-and-contact-information","title":"6. Support and Contact Information","text":"<p>Users can contact the NI-HPC support team for assistance or to report issues related to the HPC service. The HPC support team can be reached via the University Site Help Desk or by emailing hpc@qub.ac.uk. </p>"},{"location":"#7-policy-review","title":"7. Policy Review","text":"<p>This policy will be reviewed periodically and may be updated to reflect changes in the University's IT environment, legal requirements, or best practices. Users will be notified of any significant changes to this policy. </p>"},{"location":"#8-acknowledgment","title":"8. Acknowledgment","text":"<p>By using the NI-HPC service, users acknowledge that they have read, understood, and agree to comply with this Acceptable Use Policy. </p>"},{"location":"Application%20Guides/","title":"Application Guides","text":"<p>Here we will provide information on some of the most popular centrally installed applications and software tools on Kelvin2.</p>"},{"location":"Application%20Guides/#abaqus","title":"Abaqus","text":"<p>https://www.technia.com/software/simulia/abaqus</p> <p>\"The best suite of non-linear Finite Element Analysis (FEA) and Computational Fluid Dynamics (CFD) solvers.\"</p>"},{"location":"Application%20Guides/#installed-versions","title":"Installed versions","text":"<pre><code>   abaqus/2023\n   abaqus/2024\n</code></pre>"},{"location":"Application%20Guides/#usage-notes","title":"Usage notes","text":"Licensing <p>Simulia Abaqus is a licensed software. In order to use it, users have to be registered in the license server. If you are not already included in the license server, you must contact the person in charge of it:</p> <p>QUB: John Megahey smae-ithelp@qub.ac.uk</p> <p>Ulster: Jose Sanchez Bornot</p> <p>When the module is loaded, no license parameters are loaded. They should be added manually.</p> <p>To run Abaqus, it is necessary to create an environment file</p> <pre><code>   abaqus_v6.env\n</code></pre> <p>with all the necessary parameters. This file is particular to the user.</p> <p>To activate the license, the following lines should be added at the end of the environment file:</p> <pre><code>   LICENSE_SERVER_TYPE=flex\n   FLEX_LICENSE_CONFIG=&lt;port&gt;@&lt;server&gt;\n   ACADEMIC_TYPE=research\n</code></pre> <p>Contact the license administrators to get the information on the port and server.</p> User Manual <p>Visit the Simulia Abaqus web page to have access to the help material.</p> <p>And the community forum.</p>"},{"location":"Application%20Guides/#usage-examples","title":"Usage examples","text":"Abaqus batch script example <pre><code>   #!/bin/bash\n\n   #SBATCH --job-name=Cunha_LVI_DD_An_74J_Shell_R\n   #SBATCH --ntasks=20\n   #SBATCH --partition=k2-medpri\n   #SBATCH --nodes=1\n   #SBATCH --mem-per-cpu=12G\n   #SBATCH --time=23:59:00\n   #SBATCH --output=std_output_%j\n   #SBATCH --mail-user=&lt;myemail&gt;@qub.ac.uk\n   #SBATCH --mail-type=BEGIN,END,FAIL\n\n   module load abaqus/2024\n\n   unset SLURM_GTIDS\n   abaqus analysis cpus=20 job=Cunha_LVI_DD_An_74J_Shell_R double=BOTH mp_mode=mpi int\n\n   echo \"here\"\n   pwd\n</code></pre>"},{"location":"Application%20Guides/#anaconda","title":"Anaconda","text":"<p>https://www.anaconda.com/</p> <p>Anaconda is a software that allows the users to manage environments to install local libraries and software, particularly Python and R programming languages, for scientific computing.  Thus, greatly easing software management and reducing possible incompatibilities among different tools, libraries or software versions.</p>"},{"location":"Application%20Guides/#installed-versions_1","title":"Installed versions","text":"Anaconda modules<pre><code>apps/anaconda3/2024.06/bin\napps/anaconda3/2024.10/bin\n</code></pre>"},{"location":"Application%20Guides/#usage-notes_1","title":"Usage notes","text":"Setting up the environment variables <p>When the particular module for Anaconda is loaded, some environment variables must be set up in the system to make it work properly.</p> Language variables<pre><code>export LANGUAGE=en_US.UTF-8\nexport LANG=en_US.UTF-8\nexport LC_ALL=en_US.UTF-8\n</code></pre> <p>The Anaconda distributions come with a generic environment script. This script should be loaded in the environment of the user to allow all conda features, for example activating and deactivating conda environments. These environment scripts are specific to the conda version, and if an environment script is loaded for a different conda version than the one loaded in the module, it will fall into incompatibilities and will make the conda applications installed not to work.</p> <p>To load the environment scripts, it is done in <code>bash</code> and <code>ksh</code> shells preceding the name of the script with the special character dot <code>.</code>, or in csh with the key word <code>source</code>.</p> <p>The path to these environment scripts are <pre><code>&lt;Anaconda_root&gt;/bin/etc/profile.d/conda.sh\n</code></pre></p> <p>For example, in the case of the module for the version 2024.10 of Anaconda:</p> <p>bash, ksh<pre><code>. /opt/gridware/depots/54e7fb3c/el8/pkg/apps/anaconda3/2024.10/bin/etc/profile.d/conda.sh\n</code></pre> csh<pre><code>source /opt/gridware/depots/54e7fb3c/el8/pkg/apps/anaconda3/2024.10/bin/etc/profile.d/conda.sh\n</code></pre></p> <p>For users who are going to work with Anaconda in a regular basis, it is a good practice to include the definition of the language variables and the call to the environment script in the start-up script <pre><code>~/.bashrc\n</code></pre> In this case, keep in mind to modify this line in your <code>.bashrc</code> script if you change the Anaconda version for your work.</p> Redirecting default installation paths to Scratch directory <p>When installing packages with Anaconda, it is recommended that users redirect the default installation paths to their Scratch directory.  This is because the installations typically generate a very large number of small files which can breach the 100k file limit in place on the Home directory.  This can be done by modifying the environment variables <code>CONDA_PKGS_DIRS</code> and <code>CONDA_ENVS_PATH</code> as follows:</p> <pre><code>mkdir /mnt/scratch2/users/$USER/conda\nexport CONDA_PKGS_DIRS=/mnt/scratch2/users/$USER/conda/pkgs\nexport CONDA_ENVS_PATH=/mnt/scratch2/users/$USER/conda/envs\n</code></pre> Selecting the correct hardware prior to installation <p>When installing packages with Anaconda, it is critical to perform the installation on a node with the appropriate hardware in place.  For example, if you intend to run your code using a GPU device, but install that code (or its dependencies) using a node without one, the GPU device may not being recognised at runtime. This may result in your program crashing, or instead run in the CPU cores by default.  In the latter case, your jobs may take significantly longer to complete and also would also cause the requested GPU resources to be sitting idle and unavailable to other users.</p> Complexities in installing packages with Python and Anaconda <p>When installing packages with both Python and Anaconda, there may be complexities related to package version incompatibilities,  the proper Python version (maybe an older or newer version is strictly required), package installation order, or the possible necessity to install supporting libraries.  The first three \"Usage examples\" below demonstrate the installation of some commonly used applications.</p>"},{"location":"Application%20Guides/#usage-examples_1","title":"Usage examples","text":"Installing PyTorch and Ray Tune <p>In this example, PyTorch is installed together with Ray Tune.  The latter is a tool used for deep learning models optimization as it helps with the evaluation and selection of model hyperparameters  (e.g., number of layers, neurons per layer, selection between different transfer or optimization functions, etc.)</p> Pytorch-Raytune install<pre><code>srun -p k2-gpu-interactive -N 1 -n 1 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\nmodule load apps/anaconda3/2024.06/bin\nmodule load libs/nvidia-cuda/11.8.0/bin\n. /opt/gridware/depots/54e7fb3c/el8/pkg/apps/anaconda3/2024.06/bin/etc/profile.d/conda.sh\nconda create --name py39torchRayA100 python=3.9\nconda activate py39torchRayA100\n(py39torchRayA100) conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n(py39torchRayA100) python3\n(py39torchRayA100) conda install pytorch-lightning -c conda-forge\n(py39torchRayA100) conda install -c conda-forge \"ray-air\"\n(py39torchRayA100) conda deactivate\n</code></pre> <p>Line 1 above shows the use of GPU slice partitions: <code>--gres gpu:1g.10gb:1</code>.  As currently there are available 28 slices in Kelvin2, this partition is much less busy than the other GPU partitions,  and at the same time would allow the users to install software in the A100 GPU devices, which would guarantee backward compatibility with GPU devices.</p> <p>Lines 2-5 setup the environment variables, create an Anaconda environment named <code>py39torchRayA100</code> while installing Python version 3.9, and activate the environment.  This Python version is strictly required here as current Ray Tune installation version may crash with newer Python versions than 3.10.</p> <p>The next lines 6-10 are needed to install the required tools, particularly Ray Tune is installed in line 9.  Here, the package \"ray-air\" contains most of the provided functionality by Ray Tune (Data, Train, Tune, Serve, etc.).</p> <p>After installation, users must check that PyTorch can correctly utilize the GPU resources available.  For example, users must see something similar to the following output when running the PyTorch's functions:</p> Testing that torch can see GPU resources<pre><code>conda activate py39torchRayA100\n(py39torchRayA100) python3\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.cuda.is_available()\nTrue\n&gt;&gt;&gt; torch.cuda.current_device()\n0\n&gt;&gt;&gt; torch.cuda.device(0)\n&lt;torch.cuda.device object at 0x7fec6d471850&gt;\n&gt;&gt;&gt; torch.cuda.get_device_name(0)\n'NVIDIA A100-SXM4-80GB MIG 1g.10gb'\n&gt;&gt;&gt; exit()\n(py39torchRayA100) conda deactivate\n</code></pre> Installing tensorflow-gpu and Ray Tune <p>For installing tensorflow-gpu and Ray Tune, follow these instructions:</p> Tensorflow-Raytune install<pre><code>srun -p k2-gpu-interactive -N 1 -n 1 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\nmodule load apps/anaconda3/2024.06/bin\nmodule load libs/nvidia-cuda/11.8.0/bin\n. /opt/gridware/depots/54e7fb3c/el8/pkg/apps/anaconda3/2024.06/bin/etc/profile.d/conda.sh\nconda create --name tensorflowRayA100 python=3.9\nconda activate tensorflowRayA100\n(tensorflowRayA100) conda install -c anaconda tensorflow-gpu\n(tensorflowRayA100) conda install -c conda-forge \"ray-air\"\n(tensorflowRayA100) conda deactivate\n</code></pre> <p>Users also must test that tensorflow can correctly see the GPU resources. For that purpose, after installing the software, use the following commands:</p> Testing that tensorflow can see GPU resources<pre><code>conda activate tensorflowRayA100\n(tensorflowRayA100) python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n(tensorflowRayA100) conda deactivate\n</code></pre> Installing and using Bindsnet <p>In this example, users can overview how to install Bindsnet in Kelvin2:</p> Bindsnet install<pre><code>srun -p k2-gpu-interactive -N 1 -n 1 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\nmodule load apps/anaconda3/2024.06/bin\n. /opt/gridware/depots/54e7fb3c/el8/pkg/apps/anaconda3/2024.06/bin/etc/profile.d/conda.sh\nconda create --name bindsnet\nexport PATH=/mnt/scratch2/users/$USER/conda/envs/bindsnet/bin/:$PATH\nconda activate bindsnet\n(bindsnet) conda install python=3.10\n(bindsnet) python3 -m pip install git+https://github.com/BindsNET/bindsnet.git\n(bindsnet) python3 -m pip show bindsnet\n(bindsnet) python3 -m pip install pytest\n(bindsnet) conda deactivate\n</code></pre> <p>The apparent simplicity of above instructions hides some complexities.  For example, the declaration of the \"PATH\" environment variable in line 4 may be neccesary when PyPi/pip is combined with Anaconda,  as some of the installed packages may still be missing after installation, unexpectedly.  Also, in line 6, Python's version 3.10 was installed as Bindsnet most recent version at this moment seems not to be compatible with older Python versions.</p> <p>Finally, when installing Bindsnet in line 7, it automatically finds and installs all needed dependencies, including the compatible versions for torch, torchvision, etc.  Therefore, we do not recommend to install any package before bindsnet, otherwise bindsnet installation can fail.  Then, in line 8, the command <code>python3 -m pip show bindsnet</code> allows to get the physical path where Bindsnet is installed, which may be necessary to access some folders with testing examples and scripts.  For the same reasons, users may want to install the package \"pytest\", as it is done in line 9.</p> <p>As Bindsnet will install automatically PyTorch as one of its dependencies, we strongly recommend to test that PyTorch can see the GPU resources, as discussed in the Installing Pytorch and Ray Tune example.</p> <p>The following is an example of sbatch script that uses the installed Bindsnet package, which can be launched from command line using the command <code>sbatch bindsnet_example.sh</code>.  Of course, users must have first to prepare their python codes, with the main file name provided in line 19. </p> bindsnet_example.sh<pre><code>#!/bin/bash\n#SBATCH --job-name=nnet\n#SBATCH -N 1\n#SBATCH -n 4\n#SBATCH --mem=40G\n#SBATCH --partition=k2-gpu\n#SBATCH --gres gpu:v100:1\n##SBATCH --gres gpu:a100:1\n#SBATCH --time=03:00:00\n#SBATCH --output=nnet_%j.log\n\nmodule load apps/anaconda3/2024.06/bin\n. /opt/gridware/depots/54e7fb3c/el8/pkg/apps/anaconda3/2024.06/bin/etc/profile.d/conda.sh\nexport CONDA_PKGS_DIRS=/mnt/scratch2/users/$USER/conda/pkgs\nexport CONDA_ENVS_PATH=/mnt/scratch2/users/$USER/conda/envs\nexport PATH=/mnt/scratch2/users/$USER/conda/envs/bindsnet/bin/:$PATH\n\nconda activate /mnt/scratch2/users/$USER/conda/envs/bindsnet\n\npython3 &lt;code_file_name&gt;.py\n</code></pre> Installing R Packages (e.g. HOMER) with Anaconda <p>To create an environment on the user's scratch folder to install a particular version of R, follow these instructions:</p> R installation in an Anaconda environment<pre><code>srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmodule load apps/anaconda3/2024.06/bin\n. /opt/gridware/depots/54e7fb3c/el8/pkg/apps/anaconda3/2024.06/bin/etc/profile.d/conda.sh\nexport CONDA_PKGS_DIRS=/mnt/scratch2/users/$USER/conda/pkgs\nexport CONDA_ENVS_PATH=/mnt/scratch2/users/$USER/conda/envs\nconda create -n R412env -c conda-forge r-base=4.1.2\n</code></pre> <p>Here, the preferred version (4.1.2) is specified in line 5 with the argument \"r-base=4.1.2\".  For other versions, check the Anaconda R's online documentation, for example here  or here for more information.</p> <p>Installation Example: HOMER</p> <p>The following example illustrates how to use this environment for installing a particular R's package  HOMER, proceeding from a clean/new Kelvin2 terminal connection.</p> <pre><code>srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmodule load apps/anaconda3/2024.06/bin\n. /opt/gridware/depots/54e7fb3c/el8/pkg/apps/anaconda3/2024.06/bin/etc/profile.d/conda.sh\nexport CONDA_PKGS_DIRS=/mnt/scratch2/users/$USER/conda/pkgs\nexport CONDA_ENVS_PATH=/mnt/scratch2/users/$USER/conda/envs\nconda activate R412env\n(R412env)$ conda config --add channels defaults\n(R412env)$ conda config --add channels bioconda\n(R412env)$ conda config --add channels conda-forge\n(R412env)$ conda install -c bioconda samtools r-essentials bioconductor-deseq2 bioconductor-edger\n(R412env)$ conda install homer\n(R412env)$ conda update homer\n(R412env) conda deactivate\n</code></pre> <p>Users should notice that when working with R inside an Anaconda environment, the libraries must be installed with the command <code>conda install ...</code>. Whereas, the packages can be installed either in the same way by using conda install, or from the R's command line; for instance, using the function <code>installed.packages()</code>. </p> Jupyter Notebook with TensorFlow in Anaconda <p>Here, we present instructions to install and use Jupyter notebook in Kelvin2, in the case for training deep learning networks with tensorflow-gpu.</p> Jupyter notebook installation<pre><code>srun -p k2-gpu-interactive -N 1 -n 4 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\n#srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmodule load apps/anaconda3/2024.06/bin\n. /opt/gridware/depots/54e7fb3c/el8/pkg/apps/anaconda3/2024.06/bin/etc/profile.d/conda.sh\nconda create -n tf-gpu tensorflow-gpu numpy=1.19.2\n# export PATH=/mnt/scratch2/users/$USER/conda/envs/tf-gpu/bin/:$PATH\nconda activate tf-gpu\n(tf-gpu) conda install -c anaconda jupyter\n(tf-gpu) conda deactivate\n</code></pre> <p>If the user does not need to use GPU resources, for example, the plan is to run on CPU cores, then <code>srun</code> instruction in line 2 must be used, instead of line 1,  to get an interactive session in CPU nodes for the installation.  Also, ignore the installation of tensorflow part if the only purpose is to install Jupyter notebook to be used with other packages, such as Pandas or NumPy.  In that case, just create the environment in line 4 with the following instruction: <code>conda create -n &lt;environment_name&gt;</code>.</p> <p>Finally, to access Jupyter notebook remotely from the (local) browser in the user's PC/laptop, the user must launch a Jupyter server connection and create a tunnel to it.  To launch the server, run these commands:</p> <pre><code>srun -p k2-gpu-interactive -N 1 -n 4 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\n#srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmodule load apps/anaconda3/2024.06/bin\n. /opt/gridware/depots/54e7fb3c/el8/pkg/apps/anaconda3/2024.06/bin/etc/profile.d/conda.sh\nconda activate tf-gpu\n(tf-gpu) jupyter notebook --ip $(ip addr show eno1 | grep 'inet ' | awk '{print $2}' | cut -d/ -f1) --no-browser\n</code></pre> <p>Jupyter notebook has to run in a compute node. That is why first need to allocate a compute node with <code>srun</code> command as shown in line 1 or 2 for a GPU or CPU node.  Here, the server is lauched in line 5. Some critical output is printer, mainly the IP and port number, which should be annotated for using later to create the tunnel.</p> <p>For the last part, open a local terminal in the user's PC/laptop, and enter the following instructions:</p> <pre><code>ssh -p 55890 -i /path/to/kelvin/key &lt;user_name&gt;@login.kelvin.alces.network -NL 8888:10.10.15.3:8888\n</code></pre> <p>This establishes the tunnel using <code>ssh</code> command, where it has been assumed that the IP and port number annotated above are \"10.10.15.3\" and \"8888\", respectively.</p> <p>If not errors are reported during the execution of these commands, and the terminal looks like hanging out, then everything is ok,  and the last step is to open a local browser and enter the adress \"http://127.0.0.1 ...\", which must have been shown in the output when the server connection was created.</p>"},{"location":"Application%20Guides/#ansys","title":"Ansys","text":"<p>https://www.ansys.com</p> <p>\"For more than 50 years, Ansys software has enabled innovators across industries to push boundaries with the predictive power of simulation. From sustainable transportation and advanced semiconductors, to satellite systems and life-saving medical devices, the next great leaps in human advancement will be powered by Ansys.\" - Ansys Company Information</p>"},{"location":"Application%20Guides/#installed-versions_2","title":"Installed versions","text":"<pre><code>  ansys/v231/qub\n  ansys/v231/ulster\n  ansys/v241/qub\n  ansys/v241/ulster\n  ansys/v242/qub\n  ansys/v242/ulster\n  ansys/v251/qub\n  ansys/v251/ulster\n</code></pre>"},{"location":"Application%20Guides/#usage-notes_2","title":"Usage notes","text":"Licensing <p>Ansys is a licensed software. In order to use it, users have to be registered in the license server. If you are not already included in the license server, you must contact the person in charge of it:</p> <p>QUB: John Megahey smae-ithelp@qub.ac.uk</p> <p>Ulster: Jose Sanchez Bornot</p> <p>When the module is loaded, the suffixes \"qub\" and \"ulster\" specify which license-server parameters are uploaded. Users have read permission exclusively to the module file associated to their university, both have campus licence. Contact the license administrator for details.</p> User Manual <p>The user manual for Ansys is not publicly available, and only licensed users can access to it. To access to this material, you must register on the Ansys website.</p>"},{"location":"Application%20Guides/#usage-examples_2","title":"Usage examples","text":"Ansys Fluent batch script example <pre><code>#!/bin/bash\n\n#SBATCH --job-name=myfluentjob\n#SBATCH --output=myoutput.out\n#SBATCH --error=myerror.err\n#SBATCH --nodes=1\n#SBATCH --ntasks=32\n#SBATCH --partition=k2-hipri\n#SBATCH --mem=100G\n\nmodule load ansys/v241\n\n## QUB's license, already loaded with the module\n#export ANSYSLI_SERVERS=2325@143.117.212.118\n#export ANSYSLMD_LICENSE_FILE=1055@143.117.212.118\n\n## Ulster's license\n#export ANSYSLI_SERVERS=2325@193.61.145.219\n#export ANSYSLMD_LICENSE_FILE=1055@193.61.145.219\n\n# Set architecture of the CPU (in this case amd64) and environment variables\nexport FLUENT_ARCH=lnamd64\nexport FL_TMPDIR=$SCRATCH/tmp\n\n# Create our hosts file \nsrun hostname -s | sort &gt; hosts.$SLURM_JOB_ID.txt\n\n#Run Ansys Fluent.\nfluent 3ddp -g -t$SLURM_NTASKS -pinfiniband -mpi=openmpi -cnf=hosts.$SLURM_JOB_ID.txt -i my_fluent_input &gt; my_fluent_output.res\n</code></pre>"},{"location":"Application%20Guides/#matlab","title":"MATLAB","text":"<p>https://uk.mathworks.com/products/matlab.html</p> <p>\"MATLAB is a programming platform designed specifically for engineers and scientists to analyze and design systems and products that transform our world. The heart of MATLAB is the MATLAB language, a matrix-based language allowing the most natural expression of computational mathematics.\" - What is MATLAB?</p>"},{"location":"Application%20Guides/#installed-versions_3","title":"Installed versions","text":"<pre><code>   matlab/R2019a\n   matlab/R2020b\n   matlab/R2022a\n   matlab/R2024a\n</code></pre>"},{"location":"Application%20Guides/#usage-examples_3","title":"Usage examples","text":"Interactive mode (CLI) on a CPU compute node <p>One way to run MATLAB interactively is to request a compute node with the <code>srun</code> command. For example, request 1 compute node and 10 cores in \"k2-hipri\" partition:</p> <pre><code>srun -p k2-hipri -N 1 -n 10 --mem=10G --time=1:00:00 --pty bash\nmodule load matlab/R2024a\nmatlab -nosplash -nodisplay\n</code></pre> <p>Then, inside MATLAB, notice that calling the function <code>feature</code> (line #1 below) must show that exactly 10 CPU cores are available, corresponding to the number of cores allocated above with the <code>srun</code> command. Next, the codes launches the parallel pool (<code>parpool</code>) for the \"local\" cluster requesting the same amount of workers as the cores allocated in this example.</p> <pre><code>feature('numcores')\np = parpool('local', 10) % 10 cores requested in this example\n</code></pre> <p>More robust, the number of workers can be read and set automatically to launch the parallel pool:</p> <pre><code>num_workers = str2double(getenv('SLURM_CPUS_ON_NODE'))\np = parpool('local', num_workers)\n</code></pre> <p>Finally, you can run your MATLAB parallel code, which must include a \"parfor\" loop. For example, the following code illustrates the use of <code>parfor</code> and Monte Carlo simulation to calculate an approximate value for \\( \\pi \\), using the formula (line #7):</p> \\[ \\pi \\approx \\lim_{N_{MC} \\to \\infty} {4 \\sum_{n=1}^{N_{MC}} I(x_i^2 + y_i^2 &lt; 1.0) \\over N_{MC}}; x_i, y_i \\sim \\mathcal{U}(0,1), \\] <p>where the symbol \\( \\mathcal{U}(0,1) \\) represents the random uniform distribution for the indicated interval and \\( I(boolean) \\) is an indicator function.</p> Matlab code for parfor demonstration<pre><code>N = 1e6;\nout = zeros(1, num_workers);\nparfor i = 1:num_workers\n    xy = rand(N,2);\n    out(i) = sum(sum(xy.^2,2)&lt;=1);\nend\nmypi = 4*sum(out)/(N*num_workers)\n</code></pre> <p>At the end of the parallel computations, the allocated parpool within MATLAB can be released with the following command:</p> <pre><code>delete(gcp('nocreate'))\n</code></pre> Interactive mode (GUI) on a CPU compute node <p>First, get a compute node and launch vnc server from the node, for example:</p> <pre><code>srun -p k2-hipri -N 1 -n 6 --mem=10G --time=1:00:00 --pty bash\nvncserver\n</code></pre> <p>In this case, let us assume that the output of <code>vncserver</code> is</p> <p>New 'node117.pri.kelvin2.alces.network:1 (jsan)' desktop is node117.pri.kelvin2.alces.network:1</p> <p>Starting applications specified in /users/jsan/.vnc/xstartup Log file is /users/jsan/.vnc/node117.pri.kelvin2.alces.network:1.log</p> <p>Then, open a local terminal and launch a forward tunnel to the compute node by following these steps:</p> <ol> <li>Go to the directory which contains the Kelvin key in your PC/laptop <pre><code>cd /drives/c/Users/jsan/.ssh\n</code></pre></li> <li>Create the tunnel (in this example illustrated with the command below, all input sent via port 5903 on your local host is being forwarded via port 5901 to the compute node \"node117.pri.kelvin2.alces.network\". If the <code>vncserver</code> output above were \"node117.pri.kelvin2.alces.network:7\", then the port number will be 5907 instead of 5901. Clearly, users must replace the username \"jsan\" and the key's filename by the corresponding information for their accounts) <pre><code>ssh -L 5903:node117.pri.kelvin2.alces.network:5901 -p 55890 -i ./kelvin-key jsan@login.kelvin.alces.network\n</code></pre></li> <li>Connect to the tunnel using your installed VNC application (the example shown in the figure below uses TurboVNC in Windows OS. More details or troubleshooting can be found in the VNC session) </li> <li>Once in the opened terminal for the connected compute node, launch the MATLAB application: <pre><code>module load matlab/R2024a\nmatlab\n</code></pre></li> </ol> <p>This time MATLAB GUI will be opened as shown in the figure below. For a better experience, use the \"full screen\" button in the VNC toolbar, and use the combination keys Ctrl+Alt+Shift + F to escape from the full screen mode. Note also that using the instruction <code>feature('numcores')</code> inside the MATLAB GUI session shows correctly the number of allocated CPU cores in the compute node (CPU cores equal to 6 in this example).</p> <p></p> MATLAB script used in following GPU examples <p>The following code <code>matlab_gpu_example.m</code> is to be used in the following GPU examples</p> matlab_gpu_example.m<pre><code>% The Mandelbrot algorithm iterates over a grid of real and imaginary parts.\n% The following code defines the number of iterations, grid size, and grid limits.\nmaxIterations = 500;\ngridSize = 1000;\nxlim = [-0.748766713922161, -0.748766707771757];\nylim = [ 0.123640844894862,  0.123640851045266];\n\n% Use the gpuArray function to transfer data to the GPU and create a gpuArray object.\nx = gpuArray.linspace(xlim(1), xlim(2), gridSize);\ny = gpuArray.linspace(ylim(1), ylim(2), gridSize);\n\n% Many Matlab functions support gpuArrays and run directly on the GPU (e.g., meshgrid)\n% The function \"ones\", below, create an array of ones directly on the GPU\n[xGrid,yGrid] = meshgrid(x,y);\nz0 = xGrid + 1i*yGrid;\ncount = ones(size(z0), 'gpuArray');\n\n% The code below implements the Mandelbrot algorithm, fully running on the GPU\nz = z0;\nfor n = 0:maxIterations\n    z = z.*z + z0;\n    inside = abs(z) &lt;= 2;\n    count = count + inside;\nend\ncount = log(count);\n\n% Plot the results\nimagesc(x, y, count);\ncolormap([jet(); flipud(jet()); 0 0 0]);\naxis off;\n</code></pre> Interactive mode (GUI) on a GPU compute node <p>First, get a GPU node and launch vnc server from the node, for example:</p> <pre><code>srun -p k2-gpu-interactive -N 1 -n 4 --gres gpu:1g.10gb:1 --time=3:00:00 --mem=20G --pty bash\nvncserver\n</code></pre> <p>Then (as with the instructions in the 'Interactive mode (GUI) on a CPU compute node' example above),</p> <ol> <li>Open a local terminal and launch a forward tunnel to the GPU node;</li> <li>Use a VNC application to connect to the tunnel</li> <li>Load and launch the MATLAB software.</li> </ol> <p>To test the use of the GPU device within MATLAB code, copy and paste the lines from the <code>matlab_gpu_example.m</code> script file above.</p> <p>The following results should appear when you run this script in the GUI that is running on the cluster's GPU, as per below. As shown in the MATLAB command window, the output of calling the function <code>whos</code> reveal the many variables (gpuArray objects) that are still allocated on the GPU.</p> <p></p> Batch script example (GPU) <p>The most convenient way to use the cluster resources may be to perform MATLAB calculations in background, using <code>sbatch</code> functionality instead of <code>srun</code>. This is critical, mainly for analysis where calculations may take several hours or days. To demonstrate this, simply copy and paste the following \"sbatch\" script, called <code>gpu_example.sh</code>, as an example which relies on the same script prepared above to calculate the Mandelbrot solution.</p> <p>gpu_example.sh<pre><code>#!/bin/bash\n\n#SBATCH --job-name=test\n#SBATCH -N 1\n#SBATCH -n 4\n#SBATCH --mem=20G\n#SBATCH --time=00:10:00\n#SBATCH --partition=k2-gpu\n#SBATCH --gres=gpu:1g.10gb:1\n#SBATCH --output=test_%j.log\n\nmodule load matlab/R2024a\n\n# Ulster University (UU) users must use UU's licence by declaring\n# (removing comments) these environmet variables:\n#export MLM_LICENSE_FILE=27000@193.61.190.229\n#export LM_LICENSE_FILE=27000@193.61.190.229\n\nmatlab -nosplash -nodisplay -r \"matlab_gpu_example; saveas(gcf, 'Mandelbrot'); exit;\"\n</code></pre> In the code above, line #19, MATLAB is called in \"nosplash\" and \"nodisplay\" mode to execute the <code>matlab_gpu_example.m</code> script. Besides, note that the code will run in background, i.e., without display, therefore it must use some MATLAB function like <code>saveas</code>, as shown in the code, which saves the graphical output. As a result, the MATLAB file that must be listed now in the working directory, <code>Mandelbrot.fig</code>, should contain the visual results. Finally, to run this \"sbatch\" script, run the following command in your terminal.</p> <pre><code>sbatch gpu_example.sh\n</code></pre>"},{"location":"Application%20Guides/#python","title":"Python","text":"<p>https://www.python.org/</p> <p>Python is a high-level, multi-purpose programming language that support several paradigms such as structural, functional and object-oriented programming, together with garbage collection, in order to support code readability and fast development. With the provision of multiple specialized libraries, Python has become one of the most preferred languages for scientific programming, including machine learning and artificial intelligence, as well as bioinformatics among many other applications.</p>"},{"location":"Application%20Guides/#installed-versions_4","title":"Installed versions","text":"Python modules<pre><code>apps/python/2.7.17/gcc-14.1.0\napps/python3/3.10.5/gcc-14.1.0\npython3/3.10.5/gcc-9.3.0\napps/python3/3.12.4/gcc-14.1.0\n</code></pre>"},{"location":"Application%20Guides/#usage-notes_3","title":"Usage notes","text":"Anaconda environments <p>It is often recommended to use Anaconda environments to install Python packages rather than pip. This is to reduce the risk of installation problems like package incompatibilities and the user not having write permissions to Kelvin2 system paths.</p> Installing pip packages in Scratch Directory <p>It is recommended that users declare/modify the environment variables \"PATH\" and \"PYTHONPATH towards locations in the Shared Scratch directory in order to preserve space/quota in the Home directory.</p> <p><pre><code>mkdir /mnt/scratch2/users/$USER/gridware\nexport PATH=/mnt/scratch2/users/$USER/gridware/bin/:$PATH\nexport PYTHONPATH=/mnt/scratch2/users/$USER/gridware/site-packages/:$PYTHONPATH\n</code></pre> This creates and uses the folder \"gridware\" in users' Scratch directory, so the pip install will be redirected to the Scratch directory instead of the Home directory.</p> Default Python version on Kelvin2 <p>On internet blogs/forums, users will often find the recommendation <code>pip install &lt;package name&gt;</code> to install a particular tool. In this case, for the same install in Kelvin2, it is recommended to precede the command with \"python3 -m\" because by default pip will refer to the Python 2.7 version, which is always available from command line in Kelvin2. That is, always use <code>python3 -m pip install &lt;package name&gt;</code> to install the package.</p>"},{"location":"Application%20Guides/#usage-examples_4","title":"Usage examples","text":"Installing reportseff in Home directory <p>The next code snippet illustrates simply how to load a module and install a package in the default location (gridware folder located in the users' home folder). This approach is only recommended for small installs.</p> <pre><code>module load apps/python3/3.12.4/gcc-14.1.0\npython3 -m pip install reportseff\nreportseff -u $USER\n</code></pre> <p>Reportseff is a very usefull tool to monitor the efficient utilization of cluster resources (time, RAM, CPU/GPU) that may be critical for your work. It should be used in addition to <code>sacct</code> command, because it helps to optimize your jobs which may translates on significantly lower queue times. For instance, the reportseff's outcome below shows some job statistics (greed/red colour codes highlight good/poor resources utilization) for selected jobs.</p> <p></p>"},{"location":"Application%20Guides/#r","title":"R","text":"<p>https://www.r-project.org/</p> <p>R is an open-source and free software which provides a programming language designed purposedly for statistical analyses. It is also highly preferable for data mining tasks and machine learning analysis. </p>"},{"location":"Application%20Guides/#installed-versions_5","title":"Installed versions","text":"<pre><code>apps/R/4.4.1/gcc-14.1.0+openblas-0.3.27\n</code></pre>"},{"location":"Application%20Guides/#usage-notes_4","title":"Usage notes","text":"Setting R environment <p>By default, installing packages in R will fill up the users' quota (~50 GB hard disk + 100,000 files limit); therefore, it is advisable that the users setup an installation path for the packages. Below, the default install location is redirected to the user's scratch folder. The most transparent way to perform this operation is to create the file \".Renviron\" in the home folder, which will keep the necessary R's environment variables to setup correctly the settings.</p> Setup R environment<pre><code>srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmkdir /mnt/scratch2/users/$USER/R\nmkdir /mnt/scratch2/users/$USER/R/lib\ncd ~\necho \"R_LIBS_USER=/mnt/scratch2/users/$USER/R/lib\" &gt; .Renviron\necho \"R_LIBS=/mnt/scratch2/users/$USER/R/lib\" &gt;&gt; .Renviron\necho \"PKG_CONFIG_PATH=/mnt/scratch2/$USER/R/lib/pkgconfig\" &gt;&gt; .Renviron\necho \"\" &gt;&gt; .Renviron\nmodule load apps/R/4.4.1/gcc-14.1.0+openblas-0.3.27\nR\n&gt; .libPaths()\n&gt; quit()\n</code></pre> <p>In line 1, a compute node is requested to perform packages installation and work with R. Do not use login nodes to perform these operations. Lines 2-3 create the needed R's library folder. If they are not created, environment initialization may fail to be setup correctly. This operation needs to be done only one time at the very beginning of your R utilization in Kelvin2. It is neccesary to create \".Renviron\" in the home folder which is guaranteed in line 4. Then, the \".Renviron\" file is prepared in line 5-8. Notice, in line 8, this file must end with a newline character. If not, the instruction in the last line will be ignored without warning nor error. The environment file is also setup once, or every time the user wish to change the default install and library folders. Once the R's module is loaded and R is lauched in lines 9-10, the test of running the R's function <code>.libPaths()</code> must show correctly the new library path (line 11).</p>"},{"location":"Application%20Guides/#usage-examples_5","title":"Usage examples","text":"Installing R packages, e.g. BiocManager, DESeq2 <p>After setting up correctly the \".Renviron\" file as presented above, installation of R's packages can proceed inside the R's application as follows:</p> <pre><code>srun -p k2-hipri -N 1 -n 4 --time=3:00:00 --mem=16G --pty bash\nmodule load libpng/16\nmodule load apps/R/4.4.1/gcc-14.1.0+openblas-0.3.27\nexport HDF5_USE_FILE_LOCKING='FALSE'\nR\n&gt; if (!requireNamespace('BiocManager', quietly = TRUE))\n&gt; install.packages('BiocManager')\n&gt; BiocManager::install('DESeq2')\n&gt; installed.packages()\n&gt; quit()\n</code></pre> <p>Here, some issues may arise, such as that packages will not be installed correctly if some dependencies, libraries, or settings are not guaranteed. Therefore, the commands in lines 2, 4 are necessary in the present example. After that, the installation of the packages \"BiocManager\" and \"DESeq2\" is most straightforwardly done as shown in lines 7-8. Finally, the call to <code>installed.packages()</code> in line 9 will show all the installed R's packages.</p> <p>When required libraries are not available in Kelvin2 module system, the users must contact the RSE Kelvin2 team. Otherwise, they can proceed to create an Anaconda environment, where installation of the R's preferred version, libraries and packages can be performed.</p>"},{"location":"Application%20Guides/#singularity","title":"Singularity","text":"<p>In Kelvin-2, we use Singularity to run containers. The main advantage of Singularity is that it does not require root privileges to install the containers. Because of that, it is the most commonly used container in HPC systems.</p>"},{"location":"Application%20Guides/#installed-versions_6","title":"Installed versions","text":"<pre><code>   apps/singularity/3.10.0\n   apps/singularity/3.4.2\n   apps/apptainer/1.1.2\n   apps/apptainer/1.3.4\n</code></pre>"},{"location":"Application%20Guides/#usage-examples_6","title":"Usage examples","text":"<p>More detail about using singularity and containers on Kelvin-2 can be found in the online seminar</p> Running a Docker image on Singularity <p>You need the docker image file <code>mydockerimage.img</code> as a tarball <code>mytarball.tar</code></p> <p>Go to a compute node <pre><code>srun --pty --partition=k2-hipri --ntasks=1 --mem-per-cpu=2G bash\n</code></pre></p> <p>Load the module <pre><code>module load apps/singularity/3.4.2\n</code></pre></p> <p>To convert the tarball to singularity, first go to the directory where to tarball is located</p> <pre><code>cd /path/to/tarball\n</code></pre> <p>Convert the tarball:</p> <pre><code>singularity build --sandbox mytarball docker-archive://mytarball.tar\n</code></pre> <p>Execute the image</p> <pre><code>singularity shell myimage\nsingularity exec myimage mycommand\nsingularity run myimage\n</code></pre> <p>Create the tarball from a Docker image</p> <p>These steps should be done in your local computer, where you have docker. Once you create the tarball, you have to copy it to Kelvin-2 and create the Singularity image in Kelvin-2.</p> <pre><code>&lt;my_local_machine&gt;$ docker save mydockerimage -o mytarball.tar\n</code></pre>"},{"location":"Compilers/","title":"Compilers","text":"<p>Kelvin-2 has a large set of compilers and libraries for those users who compile their own self-programmed applications. We hardly recommend avoiding the system compilers, and to use those ones which are installed as modules. Modules for compilers as flagged in general as</p> <pre><code>compilers/&lt;name&gt;/&lt;version&gt;\n</code></pre> <p>And libraries</p> <pre><code>libs/&lt;name&gt;/&lt;version&gt;/&lt;compiler&gt;\n</code></pre> <p>where  states the compiler and version that it was compiled with, and in some cases, other libraries as dependencies. If you are going to use a precompiled library, be sure to use for your application the same compiler and version that the particular library was compiled with."},{"location":"Compilers/#compilers-available-on-kelvin-2","title":"Compilers available on Kelvin-2","text":"<p>On Kelvin-2, for usual programming languages as C, C++, or Fortran, we recommend using the GNU compiling suite. It is well tested, and it is currently the fastest for AMD systems. It is also a universal compiling suite, so any code will compile with it.</p> <pre><code>compilers/gcc/10.2.0\ncompilers/gcc/10.3.0\ncompilers/gcc/5.1.0\ncompilers/gcc/6.4.0\ncompilers/gcc/7.2.0\ncompilers/gcc/9.3.0\n</code></pre> <p>Other compilers installed in Kelvin-2 are the AOCC, Clang (as part of library llvm), or Nvidia nvhp (only in GPU nodes).</p> <pre><code>aoc-compiler/2.2.0\naoc-compiler/3.0.0\nllvm/12.0.0/gcc-9.3.0\nnvhpc/22.7\n</code></pre> <p>In consistence with the general practice, jobs must not be run into the login nodes, and that includes compilation.  The compilers check the hardware of the node where they are working, and they produce an executable adapted to the architecture of the node.  Login nodes have different architecture than compute nodes, so an application compiled in the login nodes will likely not work in the compute nodes.</p> <p>To compile, an interactive session should be opened in a compute node.  All the compute nodes in Kelvin-2 have the same architecture, so in general it does not matter in which one of them the compilation is performed.  High-memory nodes have more RAM memory, but the model of the RAM and the processors are the same as the standard nodes,  so a program compiled in the standard nodes will work in the high-memory nodes.</p> <p>One exception is to compile a program designed to work on the GPUs.  In this case, the session must be allocated in the k2-gpu partition, and it must be allocated a GPU resource where the application will be executed.  For compatibility, we recommend using the A100 GPUs to compile. Further details about how to compile for GPUs will be stated in the specific section.</p> <pre><code>srun --pty --partition=k2-gpu --ntasks=1 --mem-per-cpu=4G --gres gpu:a100:1 bash\n</code></pre>"},{"location":"Compilers/#example-of-compilation","title":"Example of compilation","text":"<p>hello_world.c</p> <pre><code>// Program to print Hello World using C language\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nvoid main(void)\n{\n    printf(\"Hello World... \\n\");\n}\n</code></pre> <p>Compile and execute</p> <pre><code>[&lt;user&gt;@login1 [kelvin2] ~]$ srun --pty --partition=k2-hipri --ntasks=1 --mem-per-cpu=100M bash\n[&lt;user&gt;@node162 [kelvin2] ~]$ module load compilers/gcc/9.3.0\n[&lt;user&gt;@node162 [kelvin2] ~]$ gcc -O2 -o hello_world.x hello_world.c\n[&lt;user&gt;@node162 [kelvin2] ~]$ ./hello_world.x\nHello World...\n</code></pre>"},{"location":"Compilers/#compiling-parallel-applications","title":"Compiling parallel applications","text":"<ul> <li>OpenMP</li> </ul> <p>All the compilers have integrated the libraries to execute in parallel using the Open Multi Processor protocol.  In the case of the GNU compiler, the OpenMP protocol is activated just adding the flag</p> <pre><code> -fopenmp\n</code></pre> <p>to the compilation command.</p> <p>hello_world_omp.c</p> <pre><code>// OpenMP program to print Hello World\n// using C language\n\n// OpenMP header\n#include &lt;omp.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main(int argc, char* argv[])\n{\n\n    // Beginning of parallel region\n    #pragma omp parallel\n    {\n\n        printf(\"Hello World... from thread = %d\\n\",\n               omp_get_thread_num());\n    }\n    // Ending of parallel region\n}\n</code></pre> <p>Compile and execute</p> <pre><code>[&lt;user&gt;@login1 [kelvin2] ~]$ srun --pty --partition=k2-hipri --ntasks=8 --mem-per-cpu=100M bash\n[&lt;user&gt;@node162 [kelvin2] ~]$ module load compilers/gcc/9.3.0\n[&lt;user&gt;@node162 [kelvin2] ~]$ gcc -O2 -fopenmp -o hello_world_omp.x hello_world_omp.c\n[&lt;user&gt;@node162 [kelvin2] ~]$ ./hello_world_omp.x\nHello World... from thread = 3\nHello World... from thread = 2\nHello World... from thread = 6\nHello World... from thread = 7\nHello World... from thread = 4\nHello World... from thread = 5\nHello World... from thread = 0\nHello World... from thread = 1\n</code></pre> <ul> <li>MPI</li> </ul> <p>To compile using the Message Passing Interface protocol, in Kelvin-2 it is necessary to load the modules for those libraries and change the compiling commands.  The modules that activate the MPI libraries are flagged in Kelvin-2 as</p> <pre><code> mpi/&lt;name&gt;/&lt;version&gt;/&lt;compiler&gt;\n</code></pre> <p>where  points the compiler and version that the MPI libraries were compiled with.  This is important for compatibility of the compilations, it should be used the same compiler for the application than the one that was used to compile the MPI libraries. <p>The available MPI implementations on Kelvin-2 are</p> <pre><code> mpi/mpich/3.0.4/gcc-4.8.5\n mpi/mpich/4.1.1/gcc-10.3.0\n mpi/mpich2/1.5/gcc-4.8.5\n mpi/mvapich/1.2.0-3635/gcc-4.8.5\n mpi/mvapich2/1.6/gcc-4.8.5\n mpi/openmpi/1.10.1/gcc-4.8.5\n mpi/openmpi/1.10.2/gcc-4.8.5\n mpi/openmpi/1.10.7/gcc-4.8.5\n mpi/openmpi/3.1.3/gcc-4.8.5\n mpi/openmpi/3.1.4/gcc-4.8.5\n mpi/openmpi/4.0.0/gcc-4.8.5\n mpi/openmpi/4.0.0/gcc-4.8.5+ucx-1.4.0\n mpi/openmpi/4.0.0/gcc-5.1.0\n mpi/openmpi/4.0.4/gcc-9.3.0+ucx-1.8.0\n mpi/openmpi/4.1.1/gcc-9.3.0\n</code></pre> <p>We hardly recommend using the OpenMPI compiling suite, it has been widely tested, and it works stable on Kelvin-2.  The MPICh suite has not been so deeply tested, and it is not guaranteed that applications compiled with it will work on Kelvin-2.  If you decide to use the MPICh suite, be sure to test your application before using it for production.</p> <p>To compile using MPI, the compiling commands should be changed.  For example, to compile a C program with the compiler GNU 9.3.0, the command to be used is</p> <pre><code> gcc &lt;flags&gt; -o my_executable.x my_C_program.c\n</code></pre> <p>And if we want to compile it in parallel using the MPI version 4.1.1, the compiling command should be changed to</p> <pre><code> mpicc &lt;flags&gt; -o my_executable.x my_MPI-C_program.c\n</code></pre> <p>In general, the compiling commands should be changed</p> <pre><code> gcc -&gt; mpicc\n g++ -&gt; mpicxx\n gfortran -&gt; mpifortran\n</code></pre> <p>These values are taken by default, and the command \"mpicc\" will use the GNU compiler and the MPI libraries.  If you want to use a different compiler, you have to specify it in the environment variables</p> <pre><code>OMPI_MPICC\nOMPI_MPICXX\nOMPI_FC\n</code></pre> <p>For example, to compile using the MPI library v4.1.1 with the clang compiler included in the module llvm v12.0.0, the environment variables should be defined as</p> <pre><code>export OMPI_MPICC=clang\nexport OMPI_MPICXX=clang++\nexport OMPI_FC=flang\n</code></pre> <p>and compile using the commands \"mpicc\", \"mpicxx\", \"mpifortran\".</p> <p>hello_world_mpi.c</p> <pre><code>#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n\nint main(int argc, char** argv) {\n    // Initialize the MPI environment\n    MPI_Init(NULL, NULL);\n\n    // Get the number of processes\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);\n\n    // Get the rank of the process\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);\n\n    // Get the name of the processor\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len;\n    MPI_Get_processor_name(processor_name, &amp;name_len);\n\n    // Print off a hello world message\n    printf(\"Hello world from processor %s, rank %d out of %d processors\\n\",\n           processor_name, world_rank, world_size);\n\n    // Finalize the MPI environment.\n    MPI_Finalize();\n}\n</code></pre> <p>Compile and execute</p> <pre><code>[&lt;user&gt;@login1 [kelvin2] ~]$ srun --pty --partition=k2-hipri --ntasks=8 --mem-per-cpu=100M bash\n[&lt;user&gt;@node162 [kelvin2] ~]$ module load compilers/gcc/9.3.0\n[&lt;user&gt;@node162 [kelvin2] ~]$ module load mpi/openmpi/4.0.4/gcc-9.3.0+ucx-1.8.0\n[&lt;user&gt;@node162 [kelvin2] ~]$ mpicc -O2 -o hello_world_mpi.x hello_world_mpi.c\n[&lt;user&gt;@node162 [kelvin2] ~]$ mpirun -np 8 ./hello_world_mpi.x\nHello world from processor node162.pri.kelvin2.alces.network, rank 0 out of 8 processors\nHello world from processor node162.pri.kelvin2.alces.network, rank 4 out of 8 processors\nHello world from processor node162.pri.kelvin2.alces.network, rank 5 out of 8 processors\nHello world from processor node162.pri.kelvin2.alces.network, rank 1 out of 8 processors\nHello world from processor node162.pri.kelvin2.alces.network, rank 3 out of 8 processors\nHello world from processor node162.pri.kelvin2.alces.network, rank 6 out of 8 processors\nHello world from processor node162.pri.kelvin2.alces.network, rank 7 out of 8 processors\nHello world from processor node162.pri.kelvin2.alces.network, rank 2 out of 8 processors\n</code></pre> <ul> <li>OpenACC</li> </ul> <p>Open accelerator is a feature included in the Nvidia compiler.  It can be activated adding to the compilation sequence the flag</p> <pre><code> -acc\n</code></pre> <p>This option implies an intelligent check-up of the hardware at the time of the execution.  The compiler will use the available hardware resources at that time to optimize the acceleration of the marked regions of the code as \"OpenACC\",  multiple-CPUs or GPUs will be allocated by the compiler for the execution, the user does not need to specify the resources at the time of the execution.</p> <p>OpenACC is a feature of the Nvidia compiler. On Kelvin-2, it will work only on the GPU nodes.  If you want the compiler to take advantage of the GPU accelerators, you must allocate a GPU resource when you compile the code, preferably an A100 GPU.</p> <p>More information about OpenACC, including pdf user guides can be found in the web site  https://www.openacc.org</p> <p>hello_world_openacc.c</p> <pre><code>#include &lt;stdio.h&gt;\n#ifdef _OPENACC\n#include &lt;openacc.h&gt;\n#endif\n\nint main(void) {\n#ifdef _OPENACC\n    acc_device_t devtype;\n#endif\n\n    printf(\"Hello world from OpenACC\\n\");\n#ifdef _OPENACC\n    devtype = acc_get_device_type();\n    printf(\"Number of available OpenACC devices: %d\\n\", acc_get_num_devices(devtype));\n    printf(\"Type of available OpenACC devices: %d\\n\", devtype);\n#else\n    printf(\"Code compiled without OpenACC\\n\");\n#endif\n\n    return 0;\n}\n</code></pre> <p>Compile and execute</p> <pre><code>[&lt;user&gt;@login1 [kelvin2] ~]$ srun --pty --partition=k2-gpu --ntasks=1 --mem-per-cpu=1G --gres gpu:a100:1 bash\n[&lt;user&gt;@gpu111 [kelvin2] ~]$ module load nvhpc/22.7\n[&lt;user&gt;@gpu111 [kelvin2] ~]$ nvc -O2 -acc -o hello_world_openacc.x hello_world_openacc.c\n[&lt;user&gt;@gpu111 [kelvin2] ~]$ ./hello_world_openacc.x\nHello world from OpenACC\nNumber of available OpenACC devices: 1\nType of available OpenACC devices: 4\n</code></pre>"},{"location":"Compilers/#compiling-applications-that-use-gpus","title":"Compiling applications that use GPUs","text":"<p>To compile a program designed to work on the Graphical Processing Units of Kelvin-2, it is essential that it is compiled in a GPU node, so the queue \"k2-gpu\" must be allocated. For backwards compatibility, the program must be compiled in the latest model of GPU present in the machine, in our case, the Nvidia A100 GPUs. So, when allocating the interactive session to carry out the compilation, the resource A100 should be allocated with the flag</p> <pre><code>--gres gpu:a100:1\n</code></pre> <p>During the last years, the popularity of the specific GPU-focused programming languages, such as CUDA, has decreased. This is due to the publicly-available libraries have become more and more complete, and practically any mathematical operation that can benefit of the acceleration advantages of a GPU is present on those libraries. Some examples are \"cublas\" for linear-algebra operations, and \"cufft\" for Fourier transforms. Most of the libraries designed to work in the GPUs of Kelvin-2 can be found in the module for the Nvidia CUDA drivers:</p> <pre><code>libs/nvidia-cuda/11.0.3/bin\nlibs/nvidia-cuda/11.7.0/bin\n</code></pre> <p>These libraries can be including in the executables, as usual adding the flag \"-l\" to the compilation command, for example</p> <pre><code>gcc &lt;flags&gt; -lcublas -lcufft -o my_executable.x my_GPU_program.c\n</code></pre> <p>Nevertheless, if you require a very specific operation, that is not present in the available libraries, and it is necessary to use CUDA, the Nvidia compiler can be used. Currently, the Nvidia compiler is the only one installed on Kelvin-2 that can compile CUDA code</p> <pre><code>nvhpc/22.7\nnvhpc/22.7-byo\nnvhpc/22.7-nompi\n</code></pre> <p>This compiler can recognise the sections of the code in an intelligent way, so there is no need of more flags to point that it includes CUDA routines. To compile with Nvidia compiler, just use its usual commands</p> <pre><code>nvc &lt;flags&gt; -o my_executable.x my_CUDA_C_program.c\nnvc++ &lt;flags&gt; -o my_executable.x my_CUDA_C++_program.cxx\nnvfortran &lt;flags&gt; -o my_executable.x my_CUDA_Fortran_program.for\n</code></pre>"},{"location":"Connecting%20to%20Kelvin2/","title":"Connecting to Kelvin2","text":""},{"location":"Connecting%20to%20Kelvin2/#applying-for-a-kelvin2-account","title":"Applying for a Kelvin2 account","text":"<p>Complete the application form on our website to apply for an account on Kelvin2. Please allow 48 hours for your application to be processed. If your request is successful, you will receive confirmation via email along with your account credentials.</p>"},{"location":"Connecting%20to%20Kelvin2/#connecting-to-kelvin2-using-the-terminal","title":"Connecting to Kelvin2 using the terminal","text":"<p>Connecting to Kelvin2 is done via Secure Shell Protocol (SSH) using either the terminal (command prompt) that comes preinstalled with your operating system or a separate SSH client which offers additional features, such as PuTTY or MobaXterm. This section shows you how to connect to Kelvin2 using the terminal.</p> <p>Note</p> <p>Older versions of Windows, pre Windows 10 (Autumn 2018), do not have OpenSSH installed as standard for use in their Command Prompt or PowerShell. In this case a separate SSH client such as PuTTY or MobaXterm is recommended.</p> <p>There are two ways to connect to Kelvin2 depending on whether you are inside or outside the QUB network.</p>"},{"location":"Connecting%20to%20Kelvin2/#access-from-inside-the-qub-network","title":"Access from inside the QUB network","text":"<p>If you are a QUB user and you are connecting from inside the QUB network (either from being on premises or using VPN/Remote Desktop):</p> <ul> <li>Enter the following command into your terminal. <code>&lt;username&gt;</code> is your QUB staff or student number:   <pre><code>ssh &lt;username&gt;@kelvin2.qub.ac.uk\n</code></pre></li> <li>Enter your password when requested. This is the same password associated with your QUB Active Directory.</li> <li>If you have enabled multi-factor authentication (MFA), enter the verification code provided by your authenticator app when requested.</li> </ul> <p>Warning</p> <p>It is now mandatory to have MFA enabled on your Kelvin2 account. To set up MFA, follow the instructions provided further down this page.</p>"},{"location":"Connecting%20to%20Kelvin2/#access-from-outside-the-qub-network","title":"Access from outside the QUB network","text":"<p>If you are outside the QUB network you can only connect to Kelvin2 using an SSH key pair. This steps required are as follows:</p> <ol> <li>Generate an SSH key pair (and passphrase) on your machine</li> <li>Copy the public SSH key to Kelvin2.</li> <li>Connect to Kelvin2 using the SSH key and passphrase</li> </ol> <p>Steps 1 and 2 will only need to be completed once.</p> <p>1. Generate an SSH Key Pair</p> <p>The procedure for generating an SSH key pair on your terminal varies depending on your operating system. Expand the relevant option below:</p> Mac/Linux <p>Create the SSH key pair on your machine by entering the following command into your terminal: <pre><code>ssh-keygen -t rsa -f ~/.ssh/my-kelvin-key\n</code></pre> All users MUST set a passphrase when prompted.</p> <p>This will create the SSH key pair in your .ssh directory. This key pair consists of a private key <code>my-kelvin-key</code> and a public key <code>my-kelvin-key.pub</code></p> <p>It is the contents of the public key that you will want to transfer to Kelvin2. To display the contents of this file in the terminal, enter the following command: <pre><code>cat ~/.ssh/my-kelvin-key.pub\n</code></pre></p> <p>The private key should be kept safe and should not be shared with anybody.</p> <p>Please see our video on how to set up remote access via Mac.</p> Windows 10 (Autumn 2018) or later <p>Create the SSH key pair on your machine by entering the following command into your terminal: <pre><code>ssh-keygen -t rsa\n</code></pre> Call the key <code>my-kelvin-key</code> and optionally specify the full path that you want to save it to. The default location, if none is specified, is your home directory, e.g. <code>C:\\Users\\&lt;username&gt;\\my-kelvin-key</code>.</p> <p>All users MUST set a passphrase when prompted.</p> <p>This will create the SSH key pair in the directory you have specified. This key pair consists of a private key <code>my-kelvin-key</code> and a public key <code>my-kelvin-key.pub</code></p> <p>It is the contents of the public key that you will want to transfer to Kelvin2. To display the contents of this file in the terminal, enter the following command: </p> <p><pre><code>type my-kelvin-key.pub\n</code></pre> The private key should be kept safe and should not be shared with anybody.</p> Older versions of Windows <p>Older versions of Windows do not have a built in SSH client and you will have to install one separately. Two popular choices are  PuTTY  and MobaXterm which have graphical tools to generate SSH keys called PuTTYgen and MobaKeyGen, respectively.</p> <p>2. Copy the Public SSH Key to Kelvin2</p> <p>If you are able to connect to Kelvin2 from inside the QUB network, but want to access from outside the network in future, then you are able to add the public key yourself. If you are outside the QUB network and therefore do not have access to Kelvin2,  send your public key to us, and an administrator will add it for you.</p> <p>To add your public key from inside the QUB network:</p> <ul> <li> <p>Log into Kelvin2 using your QUB Active Directory credentials.</p> </li> <li> <p>Type the following command into your terminal on Kelvin2 to open the <code>authorized_keys</code> file using the vim text editor.    <pre><code>vi ~/.ssh/authorized_keys\n</code></pre></p> </li> <li>Once vim has opened, navigate to the end of the file and access \"Insert Mode\" by pressing Shift+G then Shift+A. The public SSH key already listed in the <code>authorized_keys</code> file labeled 'Alces Clusterware HPC Cluster Key' should not be modified or deleted.</li> <li>Press 'Enter' to create a new line</li> <li>On your local computer, copy the contents of <code>my-kelvin-key.pub</code> to the clipboard</li> <li>Paste this to the end of your <code>authorized_keys</code> file in Kelvin2 by right-clicking in the vim editor</li> <li>Press Esc to re-enter \"Normal Mode\"</li> <li>Save the file and exit the vim editor by typing the following command and press 'Enter'.   <pre><code>:wq\n</code></pre></li> </ul> <p>3. Connect to Kelvin2 using the SSH Key</p> <p>After this initial set-up process is complete, you can connect to Kelvin2 as follows:</p> <ul> <li>Enter the following command into your terminal. For QUB users, <code>&lt;username&gt;</code> will be your staff or student number. For UU/EPSRC users, <code>&lt;username&gt;</code> will typically be your first initial and surname. <pre><code>ssh -p 55890 -i /path/to/ssh/private/key &lt;username&gt;@login.kelvin.alces.network\n</code></pre></li> <li>Enter your passphrase when requested. This is the passphrase that you set up when creating the SSH key pair.</li> <li>Enter the verification code provided by your authenticator app when requested. If you have not yet set up MFA, follow the instructions provided in the following section</li> </ul>"},{"location":"Connecting%20to%20Kelvin2/#multi-factor-authentication-mfa-on-kelvin2","title":"Multi-Factor Authentication (MFA) on Kelvin2","text":"<p>To enhance the security of your Kelvin2 account, you are now requested to enable Multi-Factor Authentication (MFA). This involves a one-time activation process and then entering the verification code provided by your authenticator application on your mobile device (e.g. Microsoft Authenticator) every time you connect.</p> <p>For a step by step guide on how to set up MFA, Please see our set up video.</p>"},{"location":"Connecting%20to%20Kelvin2/#enabling-multi-factor-authentication","title":"Enabling Multi-Factor Authentication","text":"<p>To enable Multi-Factor Authentication (MFA):</p> <ul> <li>Log into Kelvin2 and type the following command into your terminal to generate a QR Code and key     <pre><code>/opt/flight/bin/flight mfa generate\n</code></pre></li> <li>Using the authenticator application on your mobile device, scan the QR code or enter the key</li> <li>Your authenticator should now be generating one-time passwords to access Kelvin2</li> </ul> <p>Info</p> <p>If you lose access to your authenticator application and can no longer connect to Kelvin2, contact us and an administrator will reset your MFA settings.</p>"},{"location":"Connecting%20to%20Kelvin2/#warnings-when-reconnecting-to-kelvin2","title":"Warnings when reconnecting to Kelvin2","text":"<p>When connecting to Kelvin2 you will be directed to one of four login nodes. Depending on your local settings, you may receive a warning on your terminal when you are directed to a login node that you have not connected to before: <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@       WARNING: POSSIBLE DNS SPOOFING DETECTED!          @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nThe ECDSA host key for kelvin2.qub.ac.uk has changed,\nand the key for the corresponding IP address 143.117.27.19\nis unknown. This could either mean that\nDNS SPOOFING is happening or the IP address for the host\nand its host key have changed at the same time.\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that a host key has just been changed.\n</code></pre> To prevent this warning, navigate to the <code>known_hosts</code> file on your local computer (default for linux is <code>~/.ssh/known_hosts</code> and paste the following lines: <pre><code>kelvin2.qub.ac.uk,143.117.27.19 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBOpr/Rr+2UUve4tQPVnpEc383LCNG4El2hgmnmgN15aDm5XpE3l6qjJ4fpiOaVe386bU+79FPnG1HURvulmZocU=\nkelvin2.qub.ac.uk,143.117.27.20 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBOpr/Rr+2UUve4tQPVnpEc383LCNG4El2hgmnmgN15aDm5XpE3l6qjJ4fpiOaVe386bU+79FPnG1HURvulmZocU=\nkelvin2.qub.ac.uk,143.117.27.21 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBOpr/Rr+2UUve4tQPVnpEc383LCNG4El2hgmnmgN15aDm5XpE3l6qjJ4fpiOaVe386bU+79FPnG1HURvulmZocU=\nkelvin2.qub.ac.uk,143.117.27.22 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBOpr/Rr+2UUve4tQPVnpEc383LCNG4El2hgmnmgN15aDm5XpE3l6qjJ4fpiOaVe386bU+79FPnG1HURvulmZocU=\nkelvin2.qub.ac.uk,143.117.27.51 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBOpr/Rr+2UUve4tQPVnpEc383LCNG4El2hgmnmgN15aDm5XpE3l6qjJ4fpiOaVe386bU+79FPnG1HURvulmZocU=\n</code></pre></p>"},{"location":"Connecting%20to%20Kelvin2/#connecting-to-kelvin2-using-ssh-clients","title":"Connecting to Kelvin2 using SSH clients","text":"<p>For Windows users, the recommended way to connect to Kelvin-2 is via a SSH client. The two most popular options are MobaXterm and PuTTY.</p>"},{"location":"Connecting%20to%20Kelvin2/#mobaxterm","title":"MobaXterm","text":"<p>SCP/SFTP issues with some versions of MobaXterm</p> <p>Between v23.6 and v24.1 of MobaXterm, users have experienced problems uploading and downloading files. This problem appears resolved as of v24.2. If needed, an alternative program that provides file transfers using a graphical interface is WinSCP</p> <p>MobaXterm is a highly recommended SSH client for Windows. After installing and opening this program, follow these steps to configure the remote session:</p> <ol> <li> <p>On the initial screen, click \"Session\" on the ribbon at the top left of the screen.</p> <p></p> </li> <li> <p>On the \"Sessions\" screen, click on \"SSH\".</p> <p></p> </li> <li> <p>On the \"SSH\" screen, fill the following fields:</p> <ul> <li>Remote host: <code>kelvin2.qub.ac.uk</code> from inside the QUB network, or <code>login.kelvin.alces.network</code> from outside the QUB network.</li> <li>Tick \"Specify username\", and fill the box with your Kelvin-2 username.</li> <li>Port: <code>22</code> from inside the QUB network, or <code>55890</code> from outside the QUB network.</li> </ul> <p></p> </li> <li> <p>Click on \"Advanced SSH settings\" screen and fill the following fields:</p> <ul> <li>Tick \"X11-Forwarding\". This will allow to open graphical applications.</li> <li>For the \"SSH-browser type\", select \"SCP (Enhanced Speed)\"</li> <li>Only if you are connecting from ouside the QUB campus, tick \"Use private key\". Then click on the document icon on the right of the box, and select your private key file <code>my-kelvin-key</code>.</li> <li>Click the \"OK\" box on the bottom of the Window.</li> </ul> <p>The session will be stored for future use, and it will appear on the left part of the initial MobaXTerm window, under the tab \"Sessions\". To connect again, just click on the session.</p> <p></p> </li> </ol> <p>Note</p> <p>If MFA is enabled and \"SSH-browser type\" is set to SFTP you will be receive a verification prompt at every file transfer. For this reason, we recommend setting your \"SSH-browser\" type to SCP (Enhanced Speed)</p>"},{"location":"Connecting%20to%20Kelvin2/#putty","title":"PuTTY","text":"<p>PuTTY can be installed from the Microsoft store, or  downloaded from the web.</p> <p>To use PuTTy as a remote SSH client, the configuration is similar to MobaXterm. To begin, you will need to configure a session:</p> <p>If you are connecting from outside the QUB campus, you must convert the private key <code>my-kelvin-key</code> to the PuTTy Private Key format <code>.ppk</code>. To do so, you need to open the component \"PuTTygen\", and follow the steps</p> <ol> <li> <p>Click on \"Actions - Load an existing private key file - Load\".</p> <p></p> </li> <li> <p>When the browse window opens, change the file format from \"PuTTY Private Key Files (*.ppk)\" to \"All files (*.*)\". Then, browse in your system and select your private-key file <code>my-kelvin-key</code>. You will be asked for the passphrase.</p> </li> </ol> <p>Select \"Save private key\", and save it under the name <code>my-kelvin-key.ppk</code>.</p> <p>Now, you can configure your session in PuTTy, following the steps:</p> <ol> <li> <p>On the initial window, fill the fields:</p> <ul> <li>Host name (or IP address): <code>&lt;username&gt;@kelvin2.qub.ac.uk</code> to connect from inside the QUB campus, or <code>&lt;username&gt;@login.kelvin.alces.network</code> from outside the QUB campus.</li> <li>Port: <code>22</code> to connect from inside the QUB campus, or <code>55890</code> from outside the QUB campus.</li> <li>Connection type: click on \"SSH\".</li> <li>Saved sessions: give a name to your session and click on \"Save\".</li> </ul> <p></p> </li> <li> <p>Now click on the tab \"Connection - SSH - X11\". Tick on \"Enable X11 forwarding\", this will allow to open graphical applications.</p> <p></p> </li> <li> <p>Only if you are connecting from outside the QUB campus, click on the tab \"Connection - SSH - Auth\". At the bottom, go to the box \"Private key file for authentication\", and click on \"Browse\". Select your private-key file <code>my-kelvin-key.ppk</code>.</p> <p></p> </li> <li> <p>Go back to the tab \"Session\" and save the created session, it will appear in the big box. To connect, select your session from the box, and click on \"Open\" at the bottom of the window.</p> </li> </ol>"},{"location":"Green%20HPC/","title":"Green HPC: Sustainable Use of High Performance Computing","text":"<p>Green HPC refers to using high-performance computing systems in a carbon-efficient way, aiming to minimize environmental impact. Every user of Kelvin2 can contribute to reducing emissions by following sustainable computing practices. </p>"},{"location":"Green%20HPC/#best-practices-for-carbon-efficient-hpc-use","title":"Best Practices for Carbon-Efficient HPC Use","text":"<p>1. Run Only Meaningful Jobs</p> <p>Before submitting any job, ensure it will generate valuable results. Avoid running calculations without a clear understanding of their purpose or relevance to your research. </p> <p>2. Test Before Scaling</p> <p>Always test input files and job scripts using small or short jobs before scaling up. </p> <ul> <li> <p>Use the <code>k2-sandbox</code> queue for initial testing. </p> </li> <li> <p>This reduces the number of failed or cancelled jobs\u2014preventing unnecessary carbon emissions. </p> </li> </ul> <p>3. Optimize Job Design</p> <p>Consider whether your project goals can be achieved with fewer jobs. For instance: </p> <ul> <li> <p>Refine sampling strategies when exploring parameter spaces. </p> </li> <li> <p>Use more efficient algorithms where possible. </p> </li> </ul>"},{"location":"Green%20HPC/#sustainability-first-avoiding-emissions-is-best","title":"Sustainability First: Avoiding Emissions Is Best","text":"<p>As with many areas of sustainability, the most effective approach is carbon abatement\u2014not emitting carbon in the first place. Efficient HPC use reduces the need for offsetting emissions after they occur. </p> <p>Learn More and Broaden Your Impact </p> <p>For more guidance on reducing the carbon footprint of your digital research practices, explore the Digital Humanities Climate Coalition (DHCC) Toolkit</p> <p>For AI workloads, see the \u201cMaximal Computing\u201d section. </p> <ul> <li> <p>The toolkit also offers advice on sustainable daily digital habits. </p> </li> <li> <p>A holistic approach to sustainability\u2014beyond just HPC\u2014is crucial.</p> </li> </ul>"},{"location":"Green%20HPC/#institutional-commitment-qub-and-sustainability","title":"Institutional Commitment: QUB and Sustainability","text":"<p>Queen\u2019s University Belfast promotes sustainability in research through its Concordat for Environmental Sustainability in Research and Innovation. Learn more about these initiatives here.</p>"},{"location":"Green%20HPC/#take-action","title":"Take Action","text":"<p>By adopting these practices and thinking critically about your computational impact, you can contribute meaningfully to reducing carbon emissions in research. </p> <p>Let\u2019s make every computation count. </p>"},{"location":"Job%20Submission/","title":"Job Submission","text":""},{"location":"Job%20Submission/#ni-hpc-system-diagram","title":"NI-HPC System Diagram","text":""},{"location":"Job%20Submission/#job-handling","title":"Job Handling","text":""},{"location":"Job%20Submission/#slurm","title":"Slurm","text":"<p>Jobs on the cluster are under the control of a scheduling system, Slurm.</p> <p>Jobs are scheduled according to the currently available resources and the resources that are requested. Information on how to request resources for your job are outlined here.</p> <p>Jobs are not necessarily run in the order in which they are submitted. Jobs needing a large number of cores, memory or walltime will have to queue until the requested resources become available in. The system will run smaller jobs, that can fit in available gaps, until all of the resources that have been requested for the larger job become available. Always run jobs with the specific number of resources needed."},{"location":"Job%20Submission/#submitting-a-job","title":"Submitting a Job","text":"<p>There are 2 classes of jobs that can be ran on Kelvin2.</p> <ul> <li>Non-interactive - <code>sbatch</code></li> <li>Interactive - <code>srun</code></li> </ul>"},{"location":"Job%20Submission/#sbatch","title":"sbatch","text":"<p>Jobs are submitted via a job-script To learn more about writing a jobscript see here.</p> <p>Once you have created your jobscript you then submit it using the <code>sbatch</code> command and its name :</p> <pre><code>sbatch my_jobscript.sh\n</code></pre> <p>Once your job is submitted you will recieve a unique <code>JOBID</code>.</p>"},{"location":"Job%20Submission/#srun","title":"srun","text":"<p>srun is an interactive job - allows users to run interactive applications directly on a compute node.</p> <p>To start an interactive job :</p> <pre><code>srun --pty /bin/bash\n</code></pre> <ul> <li>Users should specify resources required to run.</li> <li>Input data is the shell session or application started.</li> <li>Output data is shown on screen or can be specified to write elsewhere. </li> </ul>"},{"location":"Job%20Submission/#queue-status","title":"Queue status","text":"<p>Once you have submitted your jobs using <code>sbatch</code> or <code>srun</code>, you can then view your queue status to see how your jobs are doing along with further information:</p> <pre><code>squeue -u &lt;username&gt;\n</code></pre> <p>Example <code>squeue</code> output :</p> <pre><code>JOBID PARTITION NAME       USER  ST TIME NODES NODELIST(REASON)\n 11    all       mpiJob    user1 PD 0:00 2     node101, node102\n 2     all       serialJob user1 R  0:02 1     node101\n</code></pre>"},{"location":"Job%20Submission/#job-states","title":"Job states","text":"<p>Once you have executed the <code>squeue</code> command take note of the current state (ST) :</p> <ul> <li>Running jobs (R) - your job is currently running in the compute.</li> <li>Queuing jobs (PD) - your job is waiting for resources to become available to run</li> <li>Failed jobs (F)- your job submission has failed and should be delted from the queue.</li> </ul>"},{"location":"Job%20Submission/#deleting-jobs","title":"Deleting jobs","text":"<p>If you need to delete a job from the current queue you can use the <code>scancel</code> command with the unique <code>JOBID</code> of the job.</p> <pre><code>scancel 8\n</code></pre> <p>Users can delete their own jobs only.</p>"},{"location":"Job%20Submission/#slurm-cheat-sheet","title":"Slurm cheat sheet","text":"<p>Common job commands </p> SGE Slurm Submit a job qsub  sbatch  Delete a job qdel  scancel  Job status (all) qstat showq squeue Job status (by job) qstat -j  squeue -j  Job status (detailed) qstat -j  scontrol show job  Show expected start time qstat -j  squeue -j  --start Start an interactive job qrsh srun --pty  /bin/bash Monitor jobs resource usage qacct -j  sacct -j  --format=\"JobID,jobname,NTasks,nodelist,CPUTime,ReqMem,MaxVMSize,Elapsed\" <p>Slurm Environmental variables</p> Variable Function SLURM_ARRAY_JOB_ID Job array's master job ID number. SLURM_ARRAY_TASK_ID Job array ID (index) number. SLURM_CLUSTER_NAME Name of the cluster on which the job is executing. SLURM_CPUS_PER_TASK Number of cpus requested per task. Only set if the --cpus-per-task option is specified. SLURM_JOB_ACCOUNT Account name associated of the job allocation. SLURM_JOB_ID The ID of the job allocation. SLURM_JOB_NAME Name of the job. SLURM_JOB_NODELIST List of nodes allocated to the job. SLURM_JOB_NUM_NODES Total number of nodes in the job's resource allocation. SLURM_JOB_PARTITION Name of the partition in which the job is running. SLURM_JOB_UID The ID of the job allocation. See SLURM_JOB_ID. Included for backwards compatibility. SLURM_JOB_USER User name of the job owner SLURM_MEM_PER_CPU Same as --mem-per-cpu SLURM_MEM_PER_NODE Same as --mem SLURM_NTASKS Same as -n, --ntasks SLURM_NTASKS_PER_NODE Number of tasks requested per node. Only set if the --ntasks-per-node option is specified. SLURM_PROCID The MPI rank (or relative process ID) of the current process. <p>More information about  Slurm commands, flags and environment variables, can be found in the  Slurm web page.</p>"},{"location":"Job%20Submission/#optimization","title":"Optimization","text":"<ul> <li>Never run jobs on the login nodes. That will seriously disturb other users who are logged in. Login nodes are never to run jobs.</li> <li>Allocate interactive sessions with <code>srun</code> if you need to run a job interactively, never use the login nodes.</li> <li>Request the necessary resources for your job. Use the job analysis tool <code>sacct</code> to check if you allocated more memory than the necessary for futures jobs with the same application.</li> <li>Don't allocate more resources than necessary, that will increase the queue time, and will waste resources of the machine that could be used by other users.</li> <li>Try to spread the allocation among several nodes, 20 CPUs and 100 Gb of memory is a reasonable amount to be allocated in a single node.</li> <li>Don't allocate a big number of CPUs or a large amount of memory in a single node.</li> <li>Give freedom to Slurm to distribute the resources among the nodes, use the flags <code>--ntasks</code> and <code>--mem-per-cpu</code> to allocate resources per CPU preferably than per node.</li> <li>Don't restrict the resources per node if possible, do not use the flags <code>--ntasks-per-node</code> nor <code>--mem</code>.</li> <li>Never allocate more resources than available in the nodes. Review the training material for information about the resources per node in each partition.</li> <li>Allocate always the correct partition, double check the resources you require, in particular the wall time, and fit it in the particular partition. Check the partition table in the training material.</li> <li>Specify an error output different to the standard output. In case the job crashes, this output has useful information about why the job failed, so how it can be fix.</li> <li>Activate email notifications.</li> </ul>"},{"location":"Kelvin2%20Hardware/","title":"Kelvin2 Hardware","text":""},{"location":"Kelvin2%20Hardware/#cpu-nodes","title":"CPU Nodes","text":"<p>AMD Nodes</p> <ul> <li>Nodes: node[101-209]</li> <li>Server: Dell PowerEdge R6525</li> <li>CPU: 2x AMD EPYC 7702 64-Core Processor</li> <li>Sockets:Cores:Threads: 2:64:1</li> <li>Memory:<ul> <li>node[101-160]: 768GiB</li> <li>node[161-182], node[186-209]: 1TiB</li> <li>node[183-185]: 960GiB</li> </ul> </li> <li>NUMA domains:<ul> <li>node[101-133,135-160]: 8</li> <li>node[134,161-209]: 2</li> </ul> </li> <li>SLURM partitions:<ul> <li>node[101-159], node[161-165], node[167-182], node[196-209]: k2-hipri, k2-medpri, k2-lowpri, k2-epsrc</li> <li>node160: k2-sandbox</li> <li>node166, node[183-195]: (reserved for specific research groups)</li> </ul> </li> </ul> <p>AMD X Series Nodes</p> <ul> <li>Nodes: node[301-302]</li> <li>Server: Dell PowerEdge R6525</li> <li>CPU: 2x AMD EPYC 7773X 64-Core Processor</li> <li>Sockets:Cores:Threads: 2:64:1</li> <li>Memory: 1TiB</li> <li>NUMA domains: 8</li> <li>SLURM partitions: k2-amd-xseries</li> </ul>"},{"location":"Kelvin2%20Hardware/#gpu-nodes","title":"GPU Nodes","text":"<p>V100 nodes</p> <ul> <li>Nodes: gpu[103-110]</li> <li>Server: Dell EMC DSS 8440</li> <li>CPU: 2x Intel Xeon Platinum 8168 CPU @ 2.70GHz</li> <li>Sockets:Cores:Threads: 2:24:1</li> <li>Memory: 512GiB</li> <li>NUMA domains: 2</li> <li>GPU: 4x Tesla V100 PCIe 32GB</li> <li>SLURM partitions: k2-gpu-v100, k2-gpu-interactive, k2-epsrc-gpu-v100</li> </ul> <p>A100 nodes</p> <ul> <li>Nodes: gpu[111-113]</li> <li>Server: Dell PowerEdge XE8545</li> <li>CPU: 2x AMD EPYC 7763 64-Core Processor</li> <li>Sockets:Cores:Threads: 2:64:1</li> <li>Memory: 1TiB</li> <li>NUMA domains: 8</li> <li>GPU: 4x Tesla A100 SXM4 80GB</li> <li>SLURM partitions: k2-gpu-a100, k2-gpu-interactive, k2-epsrc-gpu-a100</li> </ul> <p>A100 slice node</p> <ul> <li>Nodes: gpu[114]</li> <li>Server: Dell PowerEdge XE8545</li> <li>CPU: 2x AMD EPYC 7763 64-Core Processor</li> <li>Sockets:Cores:Threads: 2:64:1</li> <li>Memory: 1TiB</li> <li>NUMA domains: 8</li> <li>GPU: 4x Tesla A100 SXM4 80GB, each partitioned into 7 instances</li> <li>SLURM partitions: k2-gpu-a100mig, k2-gpu-interactive, k2-epsrc-gpu-h100</li> </ul> <p>H100 node</p> <ul> <li>Nodes: gpu[121]</li> <li>Server: Dell PowerEdge XE8640</li> <li>CPU: 2x Intel(R) Xeon(R) Platinum 8468 Processor</li> <li>Sockets:Cores:Threads: 2:96:1</li> <li>Memory: 1TiB</li> <li>NUMA domains: 8</li> <li>GPU: 4x Tesla H100 SXM5 80GB</li> <li>SLURM partitions: k2-gpu-h100, k2-gpu-interactive, k2-epsrc-gpu-h100</li> </ul> <p>MI300X node</p> <ul> <li>Nodes: gpu[123]</li> <li>Server: Dell PowerEdge XE9680</li> <li>CPU: 2x Intel(R) Xeon(R) Platinum 8470 Processor</li> <li>Sockets:Cores:Threads: 2:104:1</li> <li>Memory: 2TiB</li> <li>NUMA domains: 8</li> <li>GPU: 8 x AMD MI300x</li> <li>SLURM partitions: k2-gpu-amd, k2-gpu-interactive, k2-epsrc-gpu-amd</li> </ul> <p>MAX 1100 node</p> <ul> <li>Nodes: gpu[122]</li> <li>Server: Dell PowerEdge R760xa</li> <li>CPU: 2x Intel(R) Xeon(R) Gold 6438Y+ Processor</li> <li>Sockets:Cores:Threads: 2:64:1</li> <li>Memory: 500Gb</li> <li>NUMA domains: 8</li> <li>GPU: 4x Ponte Vecchio XT (1 Tile) Data Center GPU Max 1100</li> <li>SLURM partitions: k2-gpu-intel, k2-gpu-interactive, k2-epsrc-gpu-intel</li> </ul>"},{"location":"Kelvin2%20Hardware/#high-memory-nodes","title":"High Memory Nodes","text":"<ul> <li>Nodes: smp[106-113]</li> <li>Server: Dell PowerEdge R6525</li> <li>CPU: 2x AMD EPYC 7702 64-Core Processor</li> <li>Sockets:Cores:Threads:<ul> <li>smp[106-107]: 8:16:2</li> <li>smp[108-113]: 2:64:1</li> </ul> </li> <li>Memory: 2TiB</li> <li>NUMA domains: <ul> <li>smp[106-109]: 8</li> <li>smp[110-113]: 2</li> </ul> </li> <li>SLURM partitions: k2-himem, k2-epsrc-himem</li> </ul>"},{"location":"Kelvin2%20Hardware/#legacy-cpu-nodes","title":"Legacy CPU Nodes","text":"<p>Type 1</p> <ul> <li>Nodes: node[001-024], node[026-034], node[036-039], node[041], node[045-051], node[053-061]</li> <li>Server: ProLiant XL170r Gen9</li> <li>CPU: Intel Xeon CPU E5-2660 v3 @ 2.60GHz</li> <li>Sockets:Cores:Threads: 2:10:1</li> <li>Memory:<ul> <li>node[001-014]: 256GiB</li> <li>All except node[001-014]: 128GiB</li> </ul> </li> <li>NUMA domains: 2</li> <li>SLURM partitions: <ul> <li>all except node[061]: hipri, medpri, lowpri</li> <li>node[061]: sandbox</li> </ul> </li> </ul> <p>Type 2</p> <ul> <li>Nodes: node[071-079]</li> <li>Server: ProLiant XL170r Gen10</li> <li>CPU: Intel Xeon Gold 6126 CPU @ 2.60GHz</li> <li>Sockets:Cores:Threads: 2:12:1</li> <li>Memory:<ul> <li>node[071-073]: 384GiB</li> <li>node[074-079]: 192GiB</li> </ul> </li> <li>NUMA domains: 2</li> <li>SLURM partitions: hipri, medpri, lowpri, avx512</li> </ul>"},{"location":"Kelvin2%20Hardware/#legacy-gpu-nodes","title":"Legacy GPU Nodes","text":"<p>K4200 node</p> <ul> <li>Nodes: gpu01</li> <li>Server: ProLiant DL380 Gen9</li> <li>CPU: 2x Intel Xeon CPU E5-2640 v3 @ 2.60GHz</li> <li>Sockets:Cores:Threads: 2:8:1</li> <li>Memory: 32GiB</li> <li>NUMA domains: 2</li> <li>GPU: Quadro K4200</li> <li>SLURM partitions: gpu</li> </ul> <p>V100 node</p> <ul> <li>Nodes: gpu02</li> <li>Server: ProLiant XL270d Gen10</li> <li>CPU: 2x Intel Xeon Gold 6126 CPU @ 2.60GHz</li> <li>Sockets:Cores:Threads: 2:12:1</li> <li>Memory: 768GiB</li> <li>NUMA domains: 2</li> <li>GPU: 4x Tesla V100 SXM2 32GB</li> <li>SLURM partitions: gpu</li> </ul>"},{"location":"Kelvin2%20Hardware/#legacy-high-memory-nodes","title":"Legacy High Memory Nodes","text":"<p>Type 1</p> <ul> <li>Nodes: smp01</li> <li>Server: ProLiant DL560 Gen9</li> <li>CPU: 4x Intel Xeon CPU E5-4627 v3 @ 2.60GHz</li> <li>Sockets:Cores:Threads: 4:10:1</li> <li>Memory: 1TiB</li> <li>NUMA domains: 4</li> <li>SLURM partitions: himem</li> </ul> <p>Type 2</p> <ul> <li>Nodes: smp[03-04]</li> <li>Server: ProLiant DL360 Gen10</li> <li>CPU: 2x Intel Xeon Gold 6126 CPU @ 2.60GHz</li> <li>Sockets:Cores:Threads: 2:12:2</li> <li>Memory: 1TiB</li> <li>NUMA domains: 2</li> <li>SLURM partitions: himem</li> </ul> <p>Type 3</p> <ul> <li>Nodes: smp05</li> <li>Server: ProLiant DL360 Gen10</li> <li>CPU: 2x Intel Xeon Gold 6130 CPU @ 2.10GHz</li> <li>Sockets:Cores:Threads: 2:16:1</li> <li>Memory: 768TiB</li> <li>NUMA domains: 2</li> <li>SLURM partitions: himem</li> </ul>"},{"location":"Kelvin2%20Overview/","title":"Kelvin2 Overview","text":"<p>The NI-HPC centre hosts the Kelvin-2 research cluster. Kelvin2 runs on a Linux (Centos 7) Operating system. Below we will list the resources available.</p>"},{"location":"Kelvin2%20Overview/#compute","title":"Compute","text":"<ul> <li>96 x 128 core Dell PowerEdge R6525 compute nodes with AMD EPYC 7702 dual 64-Core Processors (786GB RAM).</li> <li>8 High memory nodes (2TB RAM).</li> <li>32 x NVIDIA Tesla v100 GPUs in 8 nodes.</li> <li>16 x NVIDIA Tesla A100 GPUs in 4 nodes.</li> <li>4 x NVIDIA Tesla H100 GPUs in 1 node.</li> <li>4 x Intel MAX 1100 GPUs in 1 node.</li> <li>8 x AMD MI300X GPUs in 1 node.</li> <li>2PB of lustre parallel file system for scratch storage.</li> </ul>"},{"location":"Kelvin2%20Overview/#storage","title":"Storage","text":"<p>There are 4 pools of storage on Kelvin2 :</p>"},{"location":"Kelvin2%20Overview/#home","title":"Home","text":"<pre><code>/users/\"username\"\n</code></pre> <ul> <li>Default place that users login to.</li> <li>50GB/100K file quota.</li> <li>No automated file deletion - therefore good to store smaller/compressed longer term data.</li> </ul>"},{"location":"Kelvin2%20Overview/#scratch","title":"Scratch","text":"<pre><code>/mnt/scratch2/users/\"username\"\n</code></pre> <ul> <li>Large, shared storage area</li> <li>No Quota</li> <li>Temporary data solution - once per month a purge will delete any data that hasnt been accessed in the previous 3 months.</li> <li>Mainly used for storing data that is neccassary to running jobs and storing their output.</li> <li>Once results are output, permanent data should be moved to longer term storage ( Home/McClayRDS ).</li> </ul>"},{"location":"Kelvin2%20Overview/#temp","title":"Temp","text":"<pre><code>/tmp\n</code></pre> <ul> <li>Local scratch disk on nodes.</li> <li>Data will be automatically deleted when session ends.</li> </ul>"},{"location":"Kelvin2%20Overview/#mcclayrds","title":"McClayRDS","text":"<pre><code>/mnt/autofs/mcclayrds-projects\n</code></pre> <ul> <li>QUB users only</li> <li>Available only from  login and data movement nodes, not compute nodes.</li> <li>Group quotas enabled.</li> <li>Replicated to secondary site.</li> <li>To apply for storage on McClayRDS, contact the Research Data Management team here.</li> </ul>"},{"location":"McClayRDS/","title":"McClayRDS","text":""},{"location":"McClayRDS/#applying-for-storage","title":"Applying For Storage","text":"<p>QUB Principal Investigators (PIs) may request access to the McClay Research Data Store (McClayRDS) for the duration of their research project.</p> <p>The storage space will be assigned to individual research projects and access is available to specified members of that research group. Access can be requested by filling in the application form here. In using McClayRDS you agree to the researcher responsibilities noted below.</p> <p>Once assigned a project code, the directory is accessible at  <code>/mnt/autofs/mcclayrds-projects/&lt;project-code&gt;</code>.</p>"},{"location":"McClayRDS/#mcclayrds-data-lifecycle","title":"McClayRDS Data Lifecycle","text":"<p>When requesting storage on McClayRDS, you specify a \"Project End Date\" and a \"Data Retention Period\". This information is used to manage when your data is archived and deleted.</p> <p>After the project end date, your data will be transferred from the active storage area into an archive where it will be stored for the specified data retention period. At the end of the data retention period your data will be permenantly deleted.</p> <p>Please note that you will be unable to directly access your data while it is in the archive. Should you wish to regain access, please contact us to transfer your data back to the active area.  </p> <p>As your project end date approaches, please ensure that all unneeded data is deleted from your storage area in preparation for archiving.</p>"},{"location":"McClayRDS/#requesting-changes","title":"Requesting Changes","text":"<p>To most effectively manage McClayRDS, we require accurate and up-to-date information on your storage requirements. Please contact us if you would like to make any changes to your project including:</p> <ul> <li>Quotas</li> <li>User access list</li> <li>Project end date</li> <li>Data retention period</li> <li>Principal Investigator (PI)</li> </ul> <p>If you are a PI who is leaving QUB before the project ends, please contact us to nominate a new PI who will be able to make future decisions on the stored data. </p>"},{"location":"McClayRDS/#research-data-responsibilities","title":"Research Data Responsibilities","text":"<p>Responsibility for managing data during any research project or programme lies with Principal Investigators (PIs) and this includes the work of taught postgraduate students. </p> <p>Responsibility for managing data is with the individual researchers (academic staff or postgraduate research students) if they are working on their own. </p> <p>When responsibility is delegated to data managers, the PI retains accountability and is responsible as data owner (and data controller when personal data are collected) on behalf of the University.</p>"},{"location":"McClayRDS/#pi-responsibilities","title":"PI Responsibilities","text":"<ul> <li>Ensure data is managed in compliance with legislative requirements and University policy including the:<ul> <li>Research Data Management Policy</li> <li>Data Protection Policy</li> <li>Information Security Policy</li> <li>Information Handling Policy</li> <li>Policy on the Ethical Approval of Research</li> </ul> </li> <li>Manage personal data in compliance with data protection legislation to include adherence with the following University policies and procedures:<ul> <li>Data Protection Policy</li> <li>Information Security Policies</li> <li>Data Protection Impact Assessment Policy and Procedures</li> <li>Data Sharing Policy</li> <li>Ethical Approval of Research including Human Participants</li> </ul> </li> <li>Where data protection cannot be fully considered as part of an ethics application, ensure that a Data Protection Impact Assessment is completed.</li> <li>Where personal data is shared with a third party to process on the University\u2019s behalf ensure that the necessary contractual arrangements are in place. </li> </ul>"},{"location":"McClayRDS/#researcher-responsibilities","title":"Researcher Responsibilities","text":"<p>While you may secure ethical approval via School or Faculty Research Ethics Committee (REC), the particularities of your research data is your responsibility.</p> <ul> <li>Ensure participants are fully informed about the type of information you are collecting about them, what it will be used for, how long it will be held, who will access it and whether identifiable or non-identifiable data will be published once the research is complete.</li> <li>Ensure documentation regarding informed consent is secured and held separately from the research data.</li> <li>Check funder terms and conditions, and meet any data sharing requirements such as standards and metadata, methods of data sharing, timeframe, and retention periods.</li> <li>Ensure retention periods for data are adhered to, including making the necessary arrangements for the transfer and management of data if you leave the University.</li> <li>Through sharing your data, you could expose or compromise your participants. It is your responsibility to ensure this does not occur. Anonymization/pseudonymization and encryption are required to de-identify data and to prevent disclosive data being revealed. This needs to be implemented wherever you are storing your data e.g. OneDrive, McClay RDS. Access controls are likely to be required for sensitive or very sensitive data.</li> <li>Ensure you have the appropriate data sharing/contract/collaboration agreements in place, if necessary.</li> <li>Identify a relevant discipline-specific or institutional repository to share your research data, if none is specified by your funder. In most cases, sharing your data will necessitate acquiring a DOI, which, generally, can only be achieved through sharing in data repository.</li> </ul>"},{"location":"Quickstart%20Guide/","title":"Quick-Start Guide","text":""},{"location":"Quickstart%20Guide/#login-nodes","title":"Login nodes","text":"<p>After connecting to Kelvin2 you will start your session in one of four login nodes as indicated in the terminal window.</p> <pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$\n</code></pre> <p>While using a login node, you are only permitted to do small tasks such as</p> <ul> <li>managing files</li> <li>small data transfers</li> <li>creating, editing and submitting jobscripts</li> <li>checking and managing jobs</li> </ul> <p>Warning</p> <p>Please do not run computationally expensive jobs on a login node. There are only 4 login nodes split between all Kelvin2 users and running high workloads here will seriously impact the other users.</p>"},{"location":"Quickstart%20Guide/#running-your-jobs-on-compute-nodes","title":"Running your jobs on compute nodes","text":"<p>A user wishing to run a job on a compute node must first write a batch script. This batch script contains specifications for the required computational resources, instructions for loading program dependencies, and commands to execute the code. Once the batch script is complete, this is submitted to a job scheduler called SLURM and the job will subsequently be queued and run on a compute node when resources become available. The scheduler ensures that all users get their fair share of system resources. </p> <p>This process is quite different to how you normally run programs on your own personal computer. As such, the following step-by-step guide explains how to run a simple Python 'Hello World' job on a compute node using a batch script submitted to the job scheduler. This walkthrough is designed to be very minimal to provide a first-time user with an overview of the process. More information is contained in the Running Jobs documentation page.</p>"},{"location":"Quickstart%20Guide/#step-by-step-guide-to-submitting-your-first-job-on-kelvin2","title":"Step-by-step guide to submitting your first job on Kelvin2","text":"<p>Click the following steps to expand:</p> 1. Create Python executable file <ul> <li>Enter the following command into your Kelvin2 terminal to create the file for our 'Hello World' program     <pre><code>vi hello-world.py\n</code></pre></li> <li>Copy the following lines of Python code into the file, then save and exit vim.      <pre><code>import os\n\nprint(\"hello world! I am running on\", os.environ['HOSTNAME'])\n</code></pre>     This code will print a hello world message and the name of the compute node the code is running on to the standard output stream. If you were running this program directly, this would be printed to your terminal.</li> </ul> 2. Create SLURM batch script <ul> <li>Enter the following command into your Kelvin2 terminal to create the batch script file that we will use to submit <code>hello-world.py</code> to the job scheduler.     <pre><code>vi hello-world-jobscript.sh\n</code></pre></li> <li> <p>Copy the following lines of code into the file, then save and exit vim.      <pre><code>#!/bin/bash\n\n#SBATCH --output=hello-world-job.output\n#SBATCH --time=00:00:10\n\nmodule load apps/python3/3.10.5/gcc-9.3.0\n\npython3 hello-world.py\n</code></pre>   In this example, the batch script simply specifies:</p> <ul> <li>the file in which the program output will be written</li> <li>a time limit after which the job will be automatically closed if it has not yet completed</li> <li>which Python module should be loaded to run the code</li> <li>a line which uses a bash command to execute our Python program</li> </ul> </li> </ul> 3. Submit the batch script <ul> <li>Enter the following command into your Kelvin2 terminal to submit the batch script to the scheduler. The job will then be queued and ran on a compute node whenever resources become available.   <pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$sbatch hello-world-jobscript.sh\nSubmitted batch job &lt;jobid&gt;\n</code></pre></li> </ul> 4. Check Status of Job <ul> <li>Enter the following command into your Kelvin2 terminal to check the status of your submitted jobs.   <pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$squeue -u &lt;username&gt;\n</code></pre></li> <li>If you were quick typing the above command, your may see your job status as Running (R) or Pending (PD). If you do not see your job in your queue it means your job is already complete. </li> </ul> 5. View output of job <ul> <li>After the job is complete, enter the following command into your Kelvin2 terminal to view the output of your program.   <pre><code>  [&lt;username&gt;@login1 [kelvin2] ~]$ cat hello-world-job.output\n  hello world! I am running on node167\n</code></pre></li> <li>You have now successfully ran a job on a compute node.</li> </ul>"},{"location":"Quickstart%20Guide/#storing-your-files-on-kelvin2","title":"Storing your files on Kelvin2","text":""},{"location":"Quickstart%20Guide/#home-directory","title":"Home Directory","text":"<p>Your home directory is found at <code>/users/$USER</code>. If you followed the step-by-step guide above, then this is where you have stored <code>hello-world.py</code>, <code>hello-world-jobscript.sh</code> and <code>hello-world-job.output</code>.</p> <p>By default, the contents of your Home directory are private. You can check permissions using the <code>ls</code> command.</p> <pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$ ls -ald $HOME\ndrwx------ 17 &lt;username&gt; clusterusers 4096 Oct 31 10:12 /users/&lt;username&gt;\n</code></pre> <p>Each user has a quota for how much data (50GB) and how many files (100k) they can store in their home directory. The limit on the number of files appears very large, however some user installations can generate lots of small files (e.g. Anaconda) which can breach this limit.  The quota can be checked using the <code>quota -s</code> command as follows: <pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$ quota -s\nDisk quotas for user &lt;username&gt; (uid &lt;userid&gt;):\n     Filesystem   space   quota   limit   grace   files   quota   limit   grace\nstorage1:/export/users\n                 36512K  51200M  76800M             292    100k    150k\n</code></pre></p> <p>In this case, the user is using 36512KB out of their 50GB disk space quota and has 292 files out of their 100k file limit quota. </p>"},{"location":"Quickstart%20Guide/#shared-scratch-directory","title":"Shared Scratch Directory","text":"<p>Users also have access to a Shared Scratch directory <code>/mnt/scratch2/users/$USER</code>.</p> <p>The Shared Scratch directory does not have a defined quota for each user and is therefore more suitable for storing larger amounts of data. However, the Shared Scratch directory has a total capacity of 2PB and therefore any data that has not been accessed in the previous 90 days is subject to deletion. </p> <p>By default, the Shared Scratch directory is more open than your Home directory. You can check the permissions using the <code>ls</code> command as follows:</p> <pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$ ls -ald /mnt/scratch2/users/$USER\ndrwxr-x--- 17 &lt;username&gt; clusterusers 4096 Sep 21 13:52 /mnt/scratch2/users/&lt;username&gt;\n</code></pre> <p>This Shared Scratch directory is typically used for</p> <ul> <li>sharing data between cluster users</li> <li>temporary storage of large amounts of data (either as input to or output from jobs)</li> <li>user installations that generate a large number of small files (e.g. using Anaconda)</li> </ul>"},{"location":"Quickstart%20Guide/#temporary-directory","title":"Temporary Directory","text":"<p>Each compute node has a local scratch directory <code>/tmp</code> for storing data during a job. Although using this storage may result in faster read/write times during a job, this data will automatically be deleted when the job completes. </p> <p>The temporary directory is typically used for </p> <ul> <li>temporary files produced during a job that are not required after job completion</li> <li>files that need to be read/written to multiple times within a job</li> </ul>"},{"location":"Quickstart%20Guide/#mcclay-research-data-storage","title":"McClay Research Data Storage","text":"<p>QUB PIs may also request access to the McClay Research Data Storage (McClayRDS) for the duration of their research project. The storage space will be assigned to individual research projects and access is shared amongst members of that research group. Access can be requested by filling in the application form here. Once assigned a project code, the directory is accessible at  <code>/mnt/autofs/mcclayrds-projects/&lt;project-code&gt;</code>. </p> <p>McClayRDS is typically used for</p> <ul> <li>storage of raw data that is actively being used during a research project</li> <li>data that is required to be stored in a secure environment, replicated to a second site.</li> </ul> <p>Note</p> <p>Data in the McClayRDS is replicated to a second site to protect against data loss from disk failure/damage. The data sync occurs every 20 mins. This data is not backed up, so data loss from user modification or deletion will not be recoverable.</p>"},{"location":"Quickstart%20Guide/#storage-summary","title":"Storage Summary","text":"Description Directory Disk Space Limit File Number Limit File Deletion Policy Home <code>/users/$USER</code> 50GB 100k No automatic file deletion Scratch <code>/mnt/scratch2/users/$USER</code> N/A N/A Deletion after 90 days without access Temp <code>/tmp</code> Disk Space on Node N/A Deletion after session ends McClayRDS <code>/mnt/autofs/mcclayrds-projects/&lt;project-code&gt;</code> Project specific N/A Deletion after specified data retention period ends"},{"location":"Quickstart%20Guide/#transferring-data-with-kelvin2","title":"Transferring Data with Kelvin2","text":""},{"location":"Quickstart%20Guide/#transfer-files-using-graphical-interfaces","title":"Transfer files using graphical interfaces","text":"<p>Files can be simply transferred to and from Kelvin2 using a 'drag and drop' interface with a graphical file browser such as MobaXterm or WinSCP.</p>"},{"location":"Quickstart%20Guide/#transfer-small-files-and-folders-using-scp","title":"Transfer small files and folders using <code>scp</code>","text":"<p>Individual files can be transferred between your local computer and Kelvin2 by entering the following commands into your local terminal:</p> <p>Uploading a file</p> <p>From Inside the QUB Network: <pre><code>scp path/to/local/file.txt &lt;username&gt;@kelvin2.qub.ac.uk:/path/on/kelvin2\n</code></pre></p> <p>From Outside the QUB Network: <pre><code>scp -P 55890 -i /path/to/ssh/private/key path/to/local/file.txt &lt;username&gt;@login.kelvin.alces.network:/path/on/kelvin2\n</code></pre></p> <p>Downloading a file</p> <p>From Inside the QUB Network: <pre><code>scp &lt;username&gt;@kelvin2.qub.ac.uk:/path/to/kelvin2/file.txt path/to/local/directory/\n</code></pre></p> <p>From Outside the QUB Network: <pre><code>scp -P 55890 -i /path/to/ssh/private/key &lt;username&gt;@login.kelvin.alces.network:/path/to/kelvin2/file.txt path/to/local/directory/\n</code></pre></p> <p>Directories and their contents can also be transferred by specifing a directory instead of a file and using <code>scp</code> with a <code>-r</code> flag (for recursive).</p> <p>Note</p> <p>When using <code>scp</code>, note that the <code>-P</code> flag to specify the port is uppercase. This is a frequent cause of confusion since the flag to specify the port when using the <code>ssh</code> command is lowercase.</p>"},{"location":"Quickstart%20Guide/#transfer-large-amounts-of-data","title":"Transfer large amounts of data","text":"<p>For large amounts of data, instead of using the login nodes as shown above, please use one of the data movement nodes as this will be faster and cause less disruption to other users. The host names of these are given below:</p> <ul> <li>dm1.kelvin.alces.network</li> <li>dm2.kelvin.alces.network</li> </ul> <p>Note</p> <p>If you are transferring a large number of small files, it is recommended to first pack the files into an \"archive\" file using tools like zip or tar. For large data transfers it may also be worth looking into alternative methods such as <code>rsync</code> which allow file transfers to be stopped and restarted if required. For further support with large date transfers, please contact us.</p>"},{"location":"Running%20Jobs/","title":"Running Jobs","text":"<p>On Kelvin2, jobs are submitted via batch script to a  scheduling system called SLURM. Jobs are not necessarily run in the order in which they are submitted and those jobs which require a large number of cores, memory or walltime will have to queue until the requested resources become available. The system will run smaller jobs, that can fit in available gaps, until all of the resources that have been requested for the larger job become available.</p> <p>Note</p> <p>To improve your experience with the queue, and that of other users, please ensure that you are not requesting excessive amounts of resources to complete your jobs. After reading the below documentation, see our tips for optimising your jobscript.</p>"},{"location":"Running%20Jobs/#batch-scripts","title":"Batch Scripts","text":"<p>A SLURM batch script typically contains four main components</p> <ul> <li>A 'shebang' <code>#!</code> which states the interpreter type, i.e. <code>#!/bin/bash</code> </li> <li>A series of <code>#SBATCH</code> directives that specify resource requirements of the job and a variety of other attributes</li> <li>Lines that load modules and set the environment</li> <li>At least one executable bash line that runs code and any associated input parameters</li> </ul> <p>An brief overview of <code>#SBATCH</code> directives and modules on Kelvin2 is given below:</p>"},{"location":"Running%20Jobs/#sbatch-directives","title":"<code>#SBATCH</code> directives","text":"<p><code>#SBATCH</code> directives specify the resources required for your job and various other attributes. The definitive list of these are in the SLURM documentation, and a few of the most commonly used ones are given below.</p> <p>Attribute Directives: e.g. <code>--job-name</code>, <code>--output</code>, <code>--error</code>, <code>--time</code>, <code>--mail-type</code>, <code>--mail-user</code></p> <p><code>--job-name</code> - Specifies a name for the job. This helps identify the job when querying your queues.</p> <p><code>--output</code> - Specifies the file in which the standard output will be written. This is the output that would have been printed to the terminal, if you were running the application directly.</p> <p><code>--error</code> - Specifies the file in which the standard errors will be written.</p> <p><code>--time</code> - Specifies a time limit for the job after which it will automatically exit, e.g. 01:30:00 for 1.5 hours, or 1-00:00:00 for 1 day.  Providing an accurate time limit will help your queued jobs start running more quickly.</p> <p><code>--mail-type</code> - Notifies the user by email when various events occur, like when the job starts or ends.</p> <p><code>--mail-user</code> - Specifies the email address to receive the notifications.</p> <pre><code>#SBATCH --job-name=my-kelvin2-job\n#SBATCH --output=my-output-file\n#SBATCH --error=my-error-file\n#SBATCH --time=01:30:00\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=&lt;user-email-address&gt;\n</code></pre> <p>Partition Directive: <code>--partition</code></p> <p>The <code>--partition</code> directive should be specified according to the resource requirements and time limit of your job. For example, if you wanted to run a job with a wall time of 3 days, you would need to use a low priority partition otherwise your job would never run, e.g.</p> <pre><code>#SBATCH --partition=k2-lowpri\n#SBATCH --time=3-00:00:00\n</code></pre> <p>The main Kelvin2 partitions along with their time limits and computational resources are shown below. The default partition, if none is specified in the batch script is k2-hipri.</p> Partition Time Limit CPU Cores per Node Memory (GB) per Node k2-hipri 3 hours 128 (94 Nodes) 773 GB (59 Nodes), 1023 GB (35 Nodes) k2-medpri 24 hours \" \" k2-lowpri N/A \" \" k2-epsrc N/A \" \" k2-himem 3 days 128 (6 Nodes), 256 (2 Nodes) 2051 GB (4 Nodes) 2063 GB (4 Nodes) k2-epsrc-himem N/A \" \" k2-gpu-&lt;model&gt; 3 days 48 (8 Nodes), 128 (7 Nodes) 514 GB (8 Nodes), 1031GB (7 Nodes) k2-epsrc-gpu-&lt;model&gt; 3 days \" \" <p>Other partitions are also available, including some for specific research groups. A comprehensive list of Kelvin2 nodes, their associated partitions and their computational resources can be found using the sinfo command.</p> <p>Resource Directives: e.g. <code>--ntasks</code>, <code>--nodes</code>, <code>--cpus-per-task</code>, <code>--mem-per-cpu</code></p> <p><code>#SBATCH</code> directives for computational resources follow the format</p> <pre><code>#SBATCH --&lt;resource-type&gt;=&lt;amount&gt;\n</code></pre> <p>MPI Application example</p> <p>If your MPI application requires 40 CPU cores spread across a maximum of 2 nodes and 80GB memory (2GB per CPU core) you would include the directives</p> <pre><code>#SBATCH --ntasks=40\n#SBATCH --mem-per-cpu=2G\n#SBATCH --nodes=2\n</code></pre> <p>OpenMP Application example</p> <p>If your OpenMP application require 40 CPU cores all on the same node and 80GB memory (2GB per CPU core) you would include the directives</p> <pre><code>#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=40\n#SBATCH --mem-per-cpu=2G\n#SBATCH --nodes=1\n</code></pre> <p>GPU Directives: <code>--gres</code></p> <p>Jobs submitted to the k2-gpu, k2-gpu-interactive (for interactive jobs only) and k2-epsrc-gpu partitions can access GPUs using the <code>--gres</code> flag by following the below template (<code>&lt;N&gt;</code> is the number of GPUs requested per node)</p> <pre><code>#SBATCH --gres=gpu:&lt;gpu-type&gt;:&lt;N&gt;\n</code></pre> Available GPU Resources Example <code>--gres</code> flag Partition &lt;model&gt; specification 32 x Tesla V100 PCIe 32Gb (4 GPUs on 8 Nodes) <code>#SBATCH --gres=gpu:v100:1</code> <code>#SBATCH --partition=k2-gpu-v100</code> 12 x Tesla A100 SXM4 80GB (4 GPUs on 3 Nodes) <code>#SBATCH --gres=gpu:a100:1</code> <code>#SBATCH --partition=k2-gpu-a100</code> 12 x CI slices of a Tesla A100 SXM4 80GB (7 slices on 4 GPUs on 1 Node) <code>#SBATCH --gres=gpu:2g.20gb:1</code> <code>#SBATCH --partition=k2-gpu-a100mig</code> 4 x Tesla H100 HBM3 80GB (4 GPUs on 1 Node) <code>#SBATCH --gres=gpu:h100:1</code> <code>#SBATCH --partition=k2-gpu-h100</code> 8 x AMD ROCm  (8 GPUs on 1 Node ) <code>#SBATCH --gres=gpu:mi300x:1</code> <code>#SBATCH --partition=k2-gpu-amd</code> 4 x Intel (4 GPUs on 1 Node) <code>#SBATCH --gres=gpu:i1100:1</code> <code>#SBATCH --partition=k2-gpu-intel</code>"},{"location":"Running%20Jobs/#loading-modules","title":"Loading Modules","text":"<p>Kelvin2 has a large repository of software installed directly on the system which can be loaded for the jobs using the batch script.</p> <p>The <code>module avail</code> command will show the list of applications that are currently installed on the central repository.</p> <pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$ module avail\n---  /opt/gridware/local/el8/etc/modules  ---\n  apps/R/4.4.1/gcc-14.1.0+openblas-0.3.27\n  apps/anaconda3/2024.06/bin\n  apps/anaconda3/2024.10/bin\n  apps/bcftools/1.21/gcc-14.1.0\n  apps/bcftools/1.21/gcc-8.5.0 *default*\n  apps/bedtools/2.31.1/gcc-14.1.0\n  ...\n</code></pre> <p>Press <code>space</code> to navigate through the list one page at a time and when done press <code>q</code> to exit.</p> <p>The most useful commands related to working with modules are:</p> <ul> <li><code>module display &lt;module&gt;</code> -  Shows information about the software</li> <li><code>module load &lt;module&gt;</code> - load the selected module, including its binaries/libraries into the user's environment</li> <li><code>module unload &lt;module&gt;</code> - removes the module from the user's environment</li> <li><code>module list</code> - show the currently loaded modules in the user's environment</li> <li><code>module purge</code> - clear all modules from the user's environment</li> </ul>"},{"location":"Running%20Jobs/#two-batch-script-examples","title":"Two Batch Script Examples","text":"<p>For illustration, example jobscripts for CPU and GPU applications are shown below, but each should be customised for your specific job. More examples are given in the Applications section.</p> CPU Application (with OpenMPI) <pre><code>#!/bin/bash\n\n#SBATCH --job-name=mpi-job-name             # Specify a name for the job\n#SBATCH --output=mpi-job-output.out         # Specify where stdout will be written\n#SBATCH --error=mpi-job-error.err           # Specify where stderr will be written\n#SBATCH --time=01:30:00                     # time limit of 1hr30min\n#SBATCH --mail-user=&lt;email address&gt;         # Specify email address for notifications\n#SBATCH --mail-type=ALL                     # Specify types of notification (eg job Begin, End)\n#SBATCH --ntasks=32                         # Number of tasks to run (for MPI tasks this is number of cores) \n#SBATCH --nodes=4                           # Maximum number of nodes on which to launch tasks \n\n#SBATCH --partition=k2-hipri                # Specify SLURM partition to queue the job\n\n\nmodule load mpi/openmpi/5.0.3/gcc-14.1.0 # Load the application and dependencies\nmodule load &lt;mpi-application&gt;\n\nmpirun &lt;application-command&gt; -n 32 [options] # Run the code\n</code></pre> GPU Application (MATLAB) <pre><code>#!/bin/bash\n\n#SBATCH --job-name=matlab-job-name\n#SBATCH --output=matlab-output.out\n#SBATCH --time=00:30:00\n#SBATCH --nodes=1\n#SBATCH --ntasks=4\n#SBATCH --mem-per-cpu=5G\n#SBATCH --partition=k2-gpu-a100mig\n#SBATCH --gres=gpu:2g.20gb:1\n\nmodule load matlab/R2024a\n\n# Ulster University (UU) users must use UU's Matlab licence by declaring\n# (removing comments) these environment variables:\n#export MLM_LICENSE_FILE=27000@193.61.190.229\n#export LM_LICENSE_FILE=27000@193.61.190.229\n\nmatlab -nosplash -nodisplay -r \"matlab_gpu_script;\"\n</code></pre>"},{"location":"Running%20Jobs/#managing-submitted-jobs","title":"Managing Submitted Jobs","text":"<p>SLURM also has commands for managing your jobs.</p>"},{"location":"Running%20Jobs/#checking-job-status","title":"Checking job status","text":"<p>You can view the status of your submitted jobs by using the <code>squeue</code> command:</p> <p><pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$ squeue -u &lt;username&gt;\n</code></pre> <pre><code>             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          &lt;jobid&gt;  k2-hipri hello-wo  &lt;username&gt;  R       0:02      1 node103\n</code></pre></p> <p>The status of your job is given by the Job State code (ST) and the nodes your job is running on (or the reason it is not running) is in the 'NODELIST(REASON)' column. Common Job State codes are:</p> State Code State Description R Running Your job is currently running. PD Pending Your job is waiting for resources to be allocated F Failed Your job submission has failed. <p>The definitive list of job state codes are given in the SLURM documentation</p>"},{"location":"Running%20Jobs/#cancelling-jobs","title":"Cancelling jobs","text":"<p>If you need to cancel a pending or running job you can use the <code>scancel</code> command: <pre><code>scancel &lt;jobid&gt;\n</code></pre></p> <p>Users can delete their own jobs only.</p>"},{"location":"Running%20Jobs/#analysing-previous-jobs-using-sacct","title":"Analysing previous jobs using <code>sacct</code>","text":"<p>Jobs you have previously ran can be analysed to check how much of the resources you had allocated were used by the program. More information may be found in the SLURM documentation.</p> <pre><code>sacct -j &lt;jobid&gt; --format=\"JobID,jobname,NTasks,nodelist,CPUTime,ReqMem,MaxVMSize,Elapsed\"\n</code></pre>"},{"location":"Running%20Jobs/#interactive-jobs","title":"Interactive Jobs","text":"<p>Interactive jobs allow the user to interact with their applications as they are running on the compute node(s). This is useful for data visualisation and for debugging programs and more complex code compilations.</p> <p>Note</p> <p>Since interactive sessions involve waiting on human input, they are generally an inefficient use of computational resources. As such, we recommend using a batch script whenever possible to run your applications automatically in the background.</p> <p>You can launch an interactive session by including the <code>srun</code> command in the terminal, followed by a list of computational resources, followed by <code>--pty bash</code>.</p> <p>For example, the follow command requests 10 CPU cores and 10GB of RAM on 1 node for a 1 hour period.</p> <p><pre><code>[&lt;username&gt;@login1 [kelvin2] ~]$ srun -p k2-hipri -N 1 -n 10 --mem-per-cpu=1G --time=1:00:00 --pty bash\n</code></pre> The job will then be queued and launched when resources are allocated on the system. Note that the domain on the terminal will change to a compute node. <pre><code>srun: job &lt;jobid&gt; queued and waiting for resources\nsrun: job &lt;jobid&gt; has been allocated resources\n[&lt;username&gt;@node103 [kelvin2] ~]$\n</code></pre></p> <p>Once connected to a login node you can load modules, set your environment and run your jobs interactively.</p>"},{"location":"Running%20Jobs/#interactive-jobs-with-graphical-applications","title":"Interactive jobs with graphical applications","text":"<p>It is possible to interact with graphical applications on Kelvin2 by launching a VNC server and creating an SSH tunnel.</p> <ol> <li> <p>Launch an interactive job on a compute node by typing the following command into your Kelvin2 terminal. Modify your resource requirements as needed.     <pre><code>[&lt;username&gt;@login3 [kelvin2] ~]$ srun -p k2-hipri -N 1 -n 10 --mem-per-cpu=1G --time=1:00:00 --pty bash\n</code></pre></p> </li> <li> <p>On the compute node, launch a vncserver (TigerVNC) by typing the following into your Kelvin2 terminal:     <pre><code>[&lt;username&gt;@node181 [kelvin2] ~]$ vncserver\n</code></pre></p> </li> <li> <p>If this is your first time launching a VNC server you will be prompted to set a password. For security reasons, this should be different to your login/SSH password for Kelvin2.     <pre><code>You will require a password to access your desktops.\n\nPassword:\nVerify:\nWould you like to enter a view-only password (y/n)? n\nA view-only password is not used\n</code></pre></p> </li> <li> <p>Once your vncserver is launched you will have information printed to your screen:     <pre><code>New 'node181.pri.kelvin2.alces.network:1 (&lt;username&gt;)' desktop is node181.pri.kelvin2.alces.network:1\n\nStarting applications specified in /users/&lt;username&gt;/.vnc/xstartup\nLog file is /users/&lt;username&gt;/.vnc/node181.pri.kelvin2.alces.network:1.log\n</code></pre></p> </li> <li> <p>Take note of the number after the compute node address (e.g. the <code>1</code> at the end of <code>node181.pri.kelvin2.alces.network:1</code>). This specifies the port number you need when creating your tunnel. Since the number here is <code>1</code>, we use port <code>5901</code>, if the number <code>7</code>, we would use port <code>5907</code>.</p> </li> <li> <p>In a separate terminal on your local computer, enter the following command to launch an SSH tunnel. Modify your node name and port numbers as appropriate. Here <code>5903</code> specifies the port on your local host and <code>5901</code> comes from the port number noted in the previous step. You will have to log in and authenticate using your usual credentials.</p> <p>From inside the QUB Network <pre><code>ssh -L 5903:node181.pri.kelvin2.alces.network:5901 &lt;username&gt;@kelvin2.qub.ac.uk\n</code></pre></p> <p>From outside the QUB Network <pre><code>ssh -L 5903:node181.pri.kelvin2.alces.network:5901 -p 55890 -i /path/to/ssh/key &lt;username&gt;@login.kelvin.alces.network\n</code></pre></p> </li> <li> <p>Now open  your VNC application and connect to localhost on port 5903 (or whichever port you chose in Step 6). You will have to enter the password set on Step 3.</p> </li> <li> <p>You will now have a graphical view of a terminal in Kelvin2. From here you may load modules and open graphical applications as required.</p> </li> <li> <p>After you have finished your session</p> <ul> <li>disconnect from your graphical session on your VNC application.</li> <li> <p>find and close the vncserver on the compute node by typing the following commands into your Kelvin2 terminal.</p> <p><pre><code>[&lt;username&gt;@node181 [kelvin2] ~]$ vncserver -list\n\nTigerVNC server sessions:\n\nX DISPLAY #     PROCESS ID\n:1              102389\n[&lt;username&gt;@node181 [kelvin2] ~]$ vncserver -kill :1\nKilling Xvnc process ID 102389\n</code></pre>     - Close the tunnel by exiting the local terminal opened in Step 6.</p> </li> </ul> </li> </ol>"},{"location":"Running%20Jobs/#checkpointing","title":"Checkpointing","text":"<p>\"Checkpointing\" means periodically saving the progress of your computations, such that your job can be paused and resumed at a later time. It is good practice to use checkpointing for jobs that take a long time to complete to ensure that you do not lose all progress if any unplanned interruptions occur, e.g. out-of-memory errors/hardware failures/partition time-limits.</p> <p>Many HPC software applications support checkpointing and instructions are typically included in their user manuals.  A few examples are included below:</p> <ul> <li>PyTorch</li> <li>GROMACS</li> <li>LAMMPS</li> <li>EPOCH</li> </ul>"},{"location":"Running%20Jobs/#tips-for-optimising-your-jobscript","title":"Tips for optimising your jobscript","text":"<ul> <li>Use the job analysis tool sacct to check if you allocated more memory than the necessary for futures jobs with the same application, e.g. <code>sacct -j &lt;job_num&gt; --format=\"JobID,jobname,NTasks,nodelist,CPUTime,ReqMem,MaxVMSize,Elapsed\"</code></li> <li>Do not allocate more resources than available in the requested nodes or you job will not run.</li> <li>When possible, try not to allocate a large number of CPUs or a large amount of memory in a single node. Instead, spread the allocation among several nodes. 20 CPUs and 100 GB of memory is a reasonable amount to be allocated in a single node.</li> <li>Give freedom to Slurm to distribute the resources among the nodes. Use the flags <code>--ntasks</code> and <code>--mem-per-cpu</code> to allocate resources per CPU, rather than <code>--ntasks-per-node</code> or <code>--mem</code>.</li> <li>Allocate always the correct partition, double check the resources you require, in particular the wall time, and fit it in the particular partition.</li> <li>Specify an error output different to the standard output. In case the job crashes, this output has useful information about why the job failed, so how it can be fix.</li> <li>Activate email notifications.</li> <li>If you are intending to run a large number of similar jobs, consider submitting a Job Array</li> </ul>"},{"location":"Running%20Jobs/#slurm-cheat-sheets","title":"Slurm cheat sheets","text":"Common job commands Description Command Submit a job sbatch  Delete a job scancel  Job status (all) squeue Job status (by job) squeue -j  Job status (detailed) scontrol show job  Show expected start time squeue -j  --start Start an interactive job srun --pty  /bin/bash Monitor jobs resource usage sacct -j  --format=\"JobID,jobname,NTasks,nodelist,CPUTime,ReqMem,MaxVMSize,Elapsed\" Slurm Environmental variables Variable Function SLURM_ARRAY_JOB_ID Job array's master job ID number. SLURM_ARRAY_TASK_ID Job array ID (index) number. SLURM_CLUSTER_NAME Name of the cluster on which the job is executing. SLURM_CPUS_PER_TASK Number of cpus requested per task. Only set if the --cpus-per-task option is specified. SLURM_JOB_ACCOUNT Account name associated of the job allocation. SLURM_JOB_ID The ID of the job allocation. SLURM_JOB_NAME Name of the job. SLURM_JOB_NODELIST List of nodes allocated to the job. SLURM_JOB_NUM_NODES Total number of nodes in the job's resource allocation. SLURM_JOB_PARTITION Name of the partition in which the job is running. SLURM_JOB_UID The ID of the job allocation. See SLURM_JOB_ID. Included for backwards compatibility. SLURM_JOB_USER User name of the job owner SLURM_MEM_PER_CPU Same as --mem-per-cpu SLURM_MEM_PER_NODE Same as --mem SLURM_NTASKS Same as -n, --ntasks SLURM_NTASKS_PER_NODE Number of tasks requested per node. Only set if the --ntasks-per-node option is specified. SLURM_PROCID The MPI rank (or relative process ID) of the current process. <p>More information about  Slurm commands, flags and environment variables, can be found in the  Slurm web page.</p>"}]}